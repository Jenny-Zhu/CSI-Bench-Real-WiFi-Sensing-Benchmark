{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jenny-Zhu/WiFiSSL/blob/main/HM3_0_DL_self_supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l34KhClRxihO"
      },
      "source": [
        "# Set Up Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsS1Nf-K9tMg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import pdb\n",
        "import h5py as h5\n",
        "!pip install einops\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import seaborn as sn\n",
        "import random\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "import copy\n",
        "!pip install mat73\n",
        "import mat73\n",
        "from torch.optim import lr_scheduler\n",
        "import argparse\n",
        "\n",
        "# Start by connecting gdrive into the google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ9CWcZRvHXs"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i_vX76gGnAt"
      },
      "outputs": [],
      "source": [
        "def generate_labels(task,file_name):\n",
        "  \"\"\" generate the class label based on the filename for different task\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  task: str\n",
        "    The classification task. Should be one value in ['HumanNonhuman', 'FourClass', 'HumanID', 'HumanMotion']\n",
        "  file_name: str\n",
        "    The filename that store the data for certain class\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  label: int\n",
        "    the result encoded class label\n",
        "\n",
        "  \"\"\"\n",
        "  assert isinstance(task, str)\n",
        "  assert isinstance(file_name, str)\n",
        "  assert task in ['HumanNonhuman', 'FourClass', 'HumanID', 'HumanMotion','ThreeClass']\n",
        "\n",
        "\n",
        "  if task == 'HumanNonhuman':\n",
        "    if 'Human' in file_name:\n",
        "      label = 1\n",
        "      print('Human labeled')\n",
        "      print(label)\n",
        "    else:\n",
        "      label = 0\n",
        "      print('Nonhuman labeled')\n",
        "      print(label)\n",
        "\n",
        "  elif task == 'FourClass':\n",
        "\n",
        "\n",
        "    # Define the labels\n",
        "    label_dict = {'Human': 0, 'Pet': 1, 'IRobot':2, 'Fan':3}\n",
        "\n",
        "    if 'Human' in file_name:\n",
        "        label=label_dict['Human']\n",
        "        print('Human labeled')\n",
        "    elif 'Pet' in file_name:\n",
        "        label=label_dict['Pet']\n",
        "        print('Pet labeled')\n",
        "    elif 'IRobot' in file_name:\n",
        "        label=label_dict['IRobot']\n",
        "        print('IRobot labeled')\n",
        "    elif 'Fan' in file_name:\n",
        "        label=label_dict['Fan']\n",
        "        print('Fan labeled')\n",
        "        print(label)\n",
        "    else:\n",
        "      print('Unrecognize class type for  ' +file_name )\n",
        "\n",
        "  elif task == 'ThreeClass':\n",
        "    # Define the labels\n",
        "    label_dict = {'Human': 0, 'Pet': 1, 'IRobot':2}\n",
        "\n",
        "    if 'Human' in file_name:\n",
        "        label=label_dict['Human']\n",
        "        print('Human labeled')\n",
        "    elif 'Pet' in file_name:\n",
        "        label=label_dict['Pet']\n",
        "        print('Pet labeled')\n",
        "    elif 'IRobot' in file_name:\n",
        "        label=label_dict['IRobot']\n",
        "        print('IRobot labeled')\n",
        "    else:\n",
        "      print('Unrecognize class type for  ' +file_name )\n",
        "\n",
        "  elif task == 'HumanID':\n",
        "    tester_list = ['Andrew', 'Brain','Brendon','Dan']\n",
        "    for ind,val in enumerate(tester_list):\n",
        "      if val in file_name:\n",
        "        label = ind+1\n",
        "        break\n",
        "    print('Unrecognize class type for  ' +file_name )\n",
        "\n",
        "  elif task == 'HumanMotion':\n",
        "    motion_list = ['Running','Sneaking','Walking']\n",
        "    for ind,val in enumerate(motion_list):\n",
        "      if val in file_name:\n",
        "        label = ind+1\n",
        "        break\n",
        "\n",
        "    print('Unrecognize class type for  ' +file_name )\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "  return label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syxpEEEqZ0kx"
      },
      "outputs": [],
      "source": [
        "class OW_dataset_class(Dataset):\n",
        "    def __init__(self, data_dir, task,  experiment, if_test,test_data):\n",
        "        self.samples = []\n",
        "        self.labels = []\n",
        "\n",
        "        # find the corresponding data directory\n",
        "        if task == 'HumanNonhuman':\n",
        "            root_dir = data_dir + \"/OW_HumanNonhuman/\"+ experiment+\"/\"\n",
        "        elif task == 'FourClass':\n",
        "            root_dir = data_dir + \"/OW_HumanNonhuman/\"+ experiment+\"/\"\n",
        "        elif task == 'ThreeClass':\n",
        "            root_dir = data_dir + \"/OW_HumanNonhuman/\"+ experiment+\"/\"\n",
        "        elif task == 'HumanID':\n",
        "            root_dir = data_dir + '/OW_HumanID/'\n",
        "        elif task == 'HumanMotion':\n",
        "            root_dir = data_dir + '/OW_HAR/'\n",
        "\n",
        "        folder_name = os.path.join(root_dir, 'train/')\n",
        "        if if_test:\n",
        "          if test_data == 1:\n",
        "            folder_name = os.path.join(root_dir, 'test/')\n",
        "          elif test_data == 2:\n",
        "            folder_name = os.path.join(root_dir, 'ssl/')\n",
        "\n",
        "\n",
        "        file_list = os.listdir(folder_name)\n",
        "\n",
        "        if test_data ==2:\n",
        "          file_list = file_list+ [i for i in file_list if i.endswith('5_half_whole.mat')]\n",
        "        else:\n",
        "          file_list = [i for i in file_list if i.endswith('5_half.mat')]\n",
        "\n",
        "        file_list  = [ x for x in file_list if \"Dan\" not in x ]\n",
        "        # load python preprocessed .npz file\n",
        "        # file_list = [i for i in file_list if i.endswith('.npz')]\n",
        "        if task == 'ThreeClass':\n",
        "          file_list  = [ x for x in file_list if \"Fan\" not in x ]\n",
        "        for file_path in file_list:\n",
        "            samples = mat73.loadmat(os.path.join(folder_name, file_path))['X']\n",
        "            # samples = samples.transpose((0,2,1))\n",
        "            # if the file list end with .npz, use np.load\n",
        "            # samples = np.load(os.path.join(folder_name, file_path))['arr_0']\n",
        "            samples_tensor = torch.from_numpy(samples).float()\n",
        "            self.samples.append(samples_tensor)\n",
        "            label = generate_labels(task, file_path)\n",
        "            for i in range(samples_tensor.shape[0]):\n",
        "              self.labels.append(label)\n",
        "\n",
        "\n",
        "\n",
        "        # self.samples = torch.stack(self.samples, dim=0)\n",
        "        self.samples = torch.unsqueeze(torch.cat(self.samples, dim=0),dim=-3)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.samples.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.samples[index], self.labels[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlwryY4avWIh"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbE3vBgk1duU"
      },
      "source": [
        "## MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD_5zssUvMDg"
      },
      "outputs": [],
      "source": [
        "class OW_MLP(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    A class to store the Multi-layer perceptron model\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    class_num:int\n",
        "      the number of classes for training models\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward(x)\n",
        "      computes output Tensors from input Tensors.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,class_num,win_len,feature_size):\n",
        "\n",
        "    super(OW_MLP,self).__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "      nn.Linear(win_len*feature_size,1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(1024,128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128,class_num)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x.view(-1,win_len*feature_size)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnR6b2wRQ6Z-"
      },
      "source": [
        "## MLP Parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxOexfxOLB7R"
      },
      "outputs": [],
      "source": [
        "class MLP_Parrallel(nn.Module):\n",
        "    def __init__(self,class_num,win_len,feature_size):\n",
        "        super(MLP_Parrallel, self).__init__()\n",
        "        self.encoder_1 = MLP_encoder(win_len,feature_size)\n",
        "        self.encoder_2 = MLP_encoder(win_len,feature_size)\n",
        "\n",
        "        self.classifier = nn.Linear(128,class_num)\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class MLP_encoder(nn.Module):\n",
        "    def __init__(self,win_len,feature_size,hidden_states = 256):\n",
        "        super(MLP_encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(1*win_len*feature_size,1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024,128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mapping = nn.Linear(128, hidden_states)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        x = x.view(-1, 1*win_len*feature_size)\n",
        "        x = self.encoder(x)\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgQ2y2hF1huC"
      },
      "source": [
        "## LeNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxULYgvCwn41"
      },
      "outputs": [],
      "source": [
        "class OW_LeNet(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    A class to store the LenNet model\n",
        "    Note: We modify LeNet5 stucture for our data input size BATCH_SIZE,1,148,300\n",
        "    ...\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    class_num:int\n",
        "      the number of classes for training models\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward(x)\n",
        "      computes output Tensors from input Tensors.\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, class_num, win_len, feature_size):\n",
        "    super(OW_LeNet, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0)\n",
        "    W_in = win_len\n",
        "    H_in = feature_size\n",
        "    kernel_size, stride, padding = 5, 1, 0\n",
        "    W_out = (W_in - kernel_size + 2 * padding) // stride + 1\n",
        "    H_out = (H_in - kernel_size + 2 * padding) // stride + 1\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    W_in, H_in = W_out, H_out\n",
        "    kernel_size, stride = 2, 2\n",
        "    W_out = (W_in - kernel_size) // stride + 1\n",
        "    H_out = (H_in - kernel_size) // stride + 1\n",
        "    self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
        "    W_in, H_in = W_out, H_out\n",
        "    kernel_size, stride, padding = 5, 1, 0\n",
        "    W_out = (W_in - kernel_size + 2 * padding) // stride + 1\n",
        "    H_out = (H_in - kernel_size + 2 * padding) // stride + 1\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "    W_in, H_in = W_out, H_out\n",
        "    kernel_size, stride = 2, 2\n",
        "    W_out = (W_in - kernel_size) // stride + 1\n",
        "    H_out = (H_in - kernel_size) // stride + 1\n",
        "    self.dim_fc1 = 16 * W_out * H_out\n",
        "    self.fc1 = nn.Linear(self.dim_fc1, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, class_num)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # print(x.shape)\n",
        "    x = self.conv1(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    x = self.pool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    x = self.pool2(x)\n",
        "    x = x.view(-1, self.dim_fc1) # Flatten the tensor\n",
        "    x = self.fc1(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = nn.functional.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEk39J3NQ_gJ"
      },
      "source": [
        "## CNN_Parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqNY3hLsicyS"
      },
      "outputs": [],
      "source": [
        "class CNN_Parrallel(nn.Module):\n",
        "    def __init__(self,class_num, win_len, feature_size):\n",
        "        super(CNN_Parrallel, self).__init__()\n",
        "        self.encoder_1 = CNN_encoder(win_len, feature_size)\n",
        "        self.encoder_2 =self.encoder_1 # CNN_encoder(win_len, feature_size)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder_1.conv_channel*self.encoder_1.conv_feat_num,128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,class_num)\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class CNN_encoder(nn.Module):\n",
        "    def __init__(self,win_len, feature_size,hidden_states = 256):\n",
        "        super(CNN_encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(1,32,(15,23),stride=9),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(32,64,3,stride=(1,3)),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(64,96,(7,3),stride=(1,3)),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "\n",
        "\n",
        "        #first layer\n",
        "        W_in = win_len\n",
        "        H_in = feature_size\n",
        "        kernel_size, stride, padding = (15,23), (9,9), 0\n",
        "        W_out = (W_in - kernel_size[0] + 2 * padding) // stride[0] + 1\n",
        "        H_out = (H_in - kernel_size[1] + 2 * padding) // stride[1] + 1\n",
        "        #second layer\n",
        "        W_in, H_in = W_out, H_out\n",
        "        kernel_size, stride, padding = (3,3), (1,3), 0\n",
        "        W_out = (W_in - kernel_size[0] + 2 * padding) // stride[0] + 1\n",
        "        H_out = (H_in - kernel_size[1] + 2 * padding) // stride[1] + 1\n",
        "\n",
        "        #third layer\n",
        "        W_in, H_in = W_out, H_out\n",
        "        kernel_size, stride, padding = (7,3), (1,3), 0\n",
        "        W_out = (W_in - kernel_size[0] + 2 * padding) // stride[0] + 1\n",
        "        H_out = (H_in - kernel_size[1] + 2 * padding) // stride[1] + 1\n",
        "\n",
        "\n",
        "        self.mapping = nn.Linear(96*W_out*H_out, hidden_states)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "        self.conv_channel = 96\n",
        "        self.conv_feat_num = W_out*H_out\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        x = self.encoder(x)\n",
        "        # classifier\n",
        "        x = x.view(-1, self.conv_channel*self.conv_feat_num)\n",
        "\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97kbVoSc1mGB"
      },
      "source": [
        "## ResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNoH6yf0Ssjs"
      },
      "outputs": [],
      "source": [
        "# Define the ResNet block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet models\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes, win_len, feature_size):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self.make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self.make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self.make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self.make_layer(512, 2, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(num_blocks-1):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, num_classes, win_len, feature_size):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self.make_layer(64, 3, stride=1)\n",
        "        self.layer2 = self.make_layer(128, 4, stride=2)\n",
        "        self.layer3 = self.make_layer(256, 6, stride=2)\n",
        "        self.layer4 = self.make_layer(512, 3, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(num_blocks-1):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class ResNet101(nn.Module):\n",
        "    def __init__(self, num_classes, win_len, feature_size):\n",
        "        super(ResNet101, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self.make_layer(64, 3, stride=1)\n",
        "        self.layer2 = self.make_layer(128, 4, stride=2)\n",
        "        self.layer3 = self.make_layer(256, 23, stride=2)\n",
        "        self.layer4 = self.make_layer(512, 3, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(num_blocks-1):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.max_pool2d(out, kernel_size=3, stride=2, padding=1)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-hvDq76G-HO"
      },
      "source": [
        "## ResNet_Parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYdnY0SijTd9"
      },
      "outputs": [],
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "\n",
        "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batch_norm3(x)\n",
        "\n",
        "        #downsample if needed\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        #add identity\n",
        "        x+=identity\n",
        "        x=self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        x = self.batch_norm2(self.conv2(x))\n",
        "\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        x += identity\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class ResNet_Parrallel(nn.Module):\n",
        "    def __init__(self,ResBlock, layer_list, num_classes):\n",
        "        super(ResNet_Parrallel, self).__init__()\n",
        "        self.encoder_1 = ResNet_encoder(ResBlock, layer_list)\n",
        "        #self.encoder_2 = ResNet_encoder(ResBlock, layer_list) # output: after BN\n",
        "        self.encoder_2 =self.encoder_1\n",
        "        self.classifier = nn.Linear(512*ResBlock.expansion, num_classes)\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class ResNet_encoder(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list,  hidden_states = 256):\n",
        "        super(ResNet_encoder, self).__init__()\n",
        "        self.reshape = nn.Sequential(\n",
        "            nn.Conv2d(1,3,(10,37),stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(3,3,kernel_size=(3,37),stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64, stride=1)\n",
        "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "\n",
        "        self.mapping = nn.Linear(512*ResBlock.expansion, hidden_states)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        x = self.reshape(x)\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        x = self.max_pool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        # classifier\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x\n",
        "\n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "\n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "\n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "def ResNet18_Parrallel(num_classes):\n",
        "    return ResNet_Parrallel(Block, [2,2,2,2], num_classes)\n",
        "def ResNet50_Parrallel(num_classes):\n",
        "    return ResNet_Parrallel(Bottleneck, [3,4,6,3], num_classes)\n",
        "def ResNet101_Parrallel( num_classes):\n",
        "    return ResNet_Parrallel(Bottleneck, [3,4,23,3], num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzsZZ-38-_27"
      },
      "source": [
        "## GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjLKHkZm_Hf8"
      },
      "outputs": [],
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(GRUNet,self).__init__()\n",
        "        self.gru = nn.GRU(win_len,64,num_layers=1)\n",
        "        self.fc = nn.Linear(64,num_classes)\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,win_len,feature_size)\n",
        "        x = x.permute(2,0,1)\n",
        "        _, ht = self.gru(x)\n",
        "        outputs = self.fc(ht[-1])\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6r8573W6dgg"
      },
      "source": [
        "## GRU_Parrallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsiM1JtN6iOw"
      },
      "outputs": [],
      "source": [
        "class GRU_Parrallel(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(GRU_Parrallel, self).__init__()\n",
        "        self.encoder_1 = GRU_encoder(win_len,feature_size)\n",
        "        self.encoder_2 = GRU_encoder(win_len,feature_size)\n",
        "\n",
        "        self.classifier = nn.Linear(64,num_classes)\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class GRU_encoder(nn.Module):\n",
        "    def __init__(self,win_len,feature_size,hidden_states = 256):\n",
        "        super(GRU_encoder, self).__init__()\n",
        "        self.encoder = nn.GRU(win_len,64,num_layers=1)\n",
        "        self.mapping = nn.Linear(64, hidden_states)\n",
        "        self.win_len = win_len\n",
        "        self.feature_size =feature_size\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        x = x.view(-1,self.win_len,self.feature_size)\n",
        "        x = x.permute(2,0,1)\n",
        "        _, ht = self.encoder(x)\n",
        "        # classifier\n",
        "        x = ht[-1]\n",
        "\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nC9sjxpBpe5"
      },
      "source": [
        "## RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3zateaPBsCt"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(RNN,self).__init__()\n",
        "        self.rnn = nn.RNN(win_len,64,num_layers=1)\n",
        "        self.fc = nn.Linear(64,num_classes)\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,win_len,feature_size)\n",
        "        x = x.permute(2,0,1)\n",
        "        _, ht = self.rnn(x)\n",
        "        outputs = self.fc(ht[-1])\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alM-dRiA3xn4"
      },
      "source": [
        "## RNN Parrallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVk1ldN632BZ"
      },
      "outputs": [],
      "source": [
        "class RNN_Parrallel(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(RNN_Parrallel, self).__init__()\n",
        "        self.encoder_1 = RNN_encoder(win_len,feature_size)\n",
        "        self.encoder_2 = RNN_encoder(win_len,feature_size)\n",
        "\n",
        "        self.classifier = nn.Linear(64,num_classes)\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class RNN_encoder(nn.Module):\n",
        "    def __init__(self,win_len,feature_size,hidden_states = 256):\n",
        "        super(RNN_encoder, self).__init__()\n",
        "        self.encoder = nn.RNN(win_len,64,num_layers=1)\n",
        "        self.mapping = nn.Linear(64, hidden_states)\n",
        "        self.win_len = win_len\n",
        "        self.feature_size =feature_size\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        x = x.view(-1,self.win_len,self.feature_size)\n",
        "        x = x.permute(2,0,1)\n",
        "        _, ht = self.encoder(x)\n",
        "        # classifier\n",
        "        x = ht[-1]\n",
        "\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV-rkuKCCV6i"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb0gGm7zCZcm"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(LSTM,self).__init__()\n",
        "        self.lstm = nn.LSTM(feature_size,64,num_layers=1,batch_first=True)\n",
        "        self.fc = nn.Linear(64,num_classes)\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,win_len,feature_size)\n",
        "        # x = x.permute(2,0,1)\n",
        "        _, (ht,ct) = self.lstm(x)\n",
        "        outputs = self.fc(ht[-1])\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(BiLSTM,self).__init__()\n",
        "        self.lstm = nn.LSTM(feature_size,64,num_layers=2,bidirectional=True,batch_first=True)\n",
        "        self.fc = nn.Linear(64,num_classes)\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1,win_len,feature_size)\n",
        "        # x = x.permute(2,0,1)\n",
        "        _, (ht,ct) = self.lstm(x)\n",
        "        outputs = self.fc(ht[-1])\n",
        "        return outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y7hHzZt5YPg"
      },
      "source": [
        "## LSTM Parrallel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sixC_Bex5Xlg"
      },
      "outputs": [],
      "source": [
        "class LSTM_Parrallel(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(LSTM_Parrallel, self).__init__()\n",
        "        self.encoder_1 = LSTM_encoder(win_len,feature_size)\n",
        "        self.encoder_2 = LSTM_encoder(win_len,feature_size)\n",
        "\n",
        "        self.classifier = nn.Linear(64,num_classes)\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class LSTM_encoder(nn.Module):\n",
        "    def __init__(self,win_len,feature_size,hidden_states = 256):\n",
        "        super(LSTM_encoder, self).__init__()\n",
        "        self.encoder = nn.LSTM(feature_size,64,num_layers=1,batch_first=True)\n",
        "        self.mapping = nn.Linear(64, hidden_states)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        x = x.view(-1,win_len,feature_size)\n",
        "        #x = x.permute(2,0,1)\n",
        "        _, (ht,ct) = self.encoder(x)\n",
        "        # classifier\n",
        "        x = ht[-1]\n",
        "\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x\n",
        "\n",
        "\n",
        "class BiLSTM_Parrallel(nn.Module):\n",
        "    def __init__(self,num_classes,win_len,feature_size):\n",
        "        super(BiLSTM_Parrallel, self).__init__()\n",
        "        self.encoder_1 = BiLSTM_encoder(win_len,feature_size)\n",
        "        self.encoder_2 = BiLSTM_encoder(win_len,feature_size)\n",
        "\n",
        "        self.classifier = nn.Linear(64,num_classes)\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class BiLSTM_encoder(nn.Module):\n",
        "    def __init__(self,win_len,feature_size,hidden_states = 256):\n",
        "        super(BiLSTM_encoder, self).__init__()\n",
        "        self.encoder = nn.LSTM(feature_size,64,num_layers=1,bidirectional=True,batch_first=True)\n",
        "        self.mapping = nn.Linear(64, hidden_states)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        x = x.view(-1,win_len,feature_size)\n",
        "        #x = x.permute(2,0,1)\n",
        "        _, (ht,ct) = self.encoder(x)\n",
        "        # classifier\n",
        "        x = ht[-1]\n",
        "\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aigxPGDlDiTw"
      },
      "source": [
        "## CNN+GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLPjMT1N5Fqx"
      },
      "outputs": [],
      "source": [
        "class CNN_GRU(nn.Module):\n",
        "    def __init__(self, num_classes,win_len,feature_size):\n",
        "        super(CNN_GRU, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, (12, 6)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "            nn.Conv2d(16, 32, (7, 3)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mean = nn.AdaptiveAvgPool2d((1, None))  # Updated average pooling layer\n",
        "        self.gru = nn.GRU(113, 128, num_layers=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = len(x)\n",
        "        # x is already in the shape batch_size x 1 x 250 x 148\n",
        "        x = self.encoder(x)\n",
        "        # batch x 32 x 113 x 141\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        # batch x 141 x 32 x 113\n",
        "        x = x.reshape(batch_size * 141, 32, 113)\n",
        "        # (batch x 141) x 32 x 113\n",
        "        x = self.mean(x)\n",
        "        # print(x.shape)\n",
        "        # (batch x 141) x 1 x 113\n",
        "        x = x.squeeze(1)\n",
        "        # print(x.shape)\n",
        "        # (batch x 141) x 113\n",
        "        x = x.reshape(batch_size, 141, 113)\n",
        "        # batch x 141 x 113\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # 141 x batch x 113\n",
        "        _, ht = self.gru(x)\n",
        "        outputs = self.classifier(ht[-1])\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5LScBoy8B8G"
      },
      "source": [
        "## CNN+GRU Parrallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ACGp_xW8CQp"
      },
      "outputs": [],
      "source": [
        "class CNN_GRU_Parrallel(nn.Module):\n",
        "    def __init__(self, num_classes,win_len,feature_size):\n",
        "        super(CNN_GRU_Parrallel, self).__init__()\n",
        "        self.encoder_1 = CNN_GRU_encoder(win_len,feature_size)\n",
        "        self.encoder_2 = CNN_GRU_encoder(win_len,feature_size)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128,num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder_1(x1, flag=flag)\n",
        "            x2 = self.encoder_2(x2, flag=flag)\n",
        "            y1 = self.classifier(x1)\n",
        "            y2 = self.classifier(x2)\n",
        "            return y1, y2\n",
        "        x1 = self.encoder_1(x1)\n",
        "        x2 = self.encoder_2(x2)\n",
        "        return x1, x2\n",
        "\n",
        "class CNN_GRU_encoder(nn.Module):\n",
        "    def __init__(self,win_len,feature_size,hidden_states = 256):\n",
        "        super(CNN_GRU_encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, (12, 6)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 1)),\n",
        "            nn.Conv2d(16, 32, (7, 3)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        #self.mean = nn.AvgPool1d(32)\n",
        "        self.mean = nn.AdaptiveAvgPool2d((1, None))  # Updated average pooling layer\n",
        "        self.gru = nn.GRU(113,128,num_layers=1)\n",
        "\n",
        "        self.mapping = nn.Linear(128, hidden_states)\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(hidden_states)\n",
        "\n",
        "\n",
        "    def forward(self, x, flag='unsupervised'):\n",
        "        batch_size = len(x)\n",
        "        # # batch x 3 x 114 x 500\n",
        "        # x = x.view(batch_size,3*114,500)\n",
        "        # x = x.permute(0,2,1)\n",
        "        # # batch x 500 x 342\n",
        "        # x = x.reshape(batch_size*500,1, 3*114)\n",
        "        # # (batch x 500) x 1 x 342\n",
        "        # x = self.encoder(x)\n",
        "        # # (batch x 500) x 32 x 8\n",
        "        # # try 32, (32x8)\n",
        "        # x = x.permute(0,2,1)\n",
        "        # x = self.mean(x)\n",
        "        # x = x.reshape(batch_size, 500, 8)\n",
        "        # # batch x 500 x 8\n",
        "        # x = x.permute(1,0,2)\n",
        "        # # 500 x batch x 8\n",
        "        # _, ht = self.gru(x)\n",
        "        # # classifier\n",
        "        # x = ht[-1]\n",
        "\n",
        "        # x is already in the shape batch_size x 1 x 250 x 148\n",
        "        x = self.encoder(x)\n",
        "        # batch x 32 x 113 x 141\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        # batch x 141 x 32 x 113\n",
        "        x = x.reshape(batch_size * 141, 32, 113)\n",
        "        # (batch x 141) x 32 x 113\n",
        "        x = self.mean(x)\n",
        "        # print(x.shape)\n",
        "        # (batch x 141) x 1 x 113\n",
        "        x = x.squeeze(1)\n",
        "        # print(x.shape)\n",
        "        # (batch x 141) x 113\n",
        "        x = x.reshape(batch_size, 141, 113)\n",
        "        # batch x 141 x 113\n",
        "        x = x.permute(1, 0, 2)\n",
        "        # 141 x batch x 113\n",
        "        _, ht = self.gru(x)\n",
        "        x = ht[-1]\n",
        "\n",
        "        if flag == 'supervised':\n",
        "            return x\n",
        "        else:\n",
        "            x = self.bn(self.mapping(x))\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SKISJ3JAy5f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe_Qzx-3EKV3"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj1RtSngPQbM"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, win_len, feature_size, emb_size, in_channels = 1):\n",
        "        self.win_len = win_len\n",
        "        self.feature_size = feature_size\n",
        "        patch_size_w = int(win_len/10)\n",
        "        patch_size_h = int(feature_size/4)\n",
        "        img_size = win_len*feature_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size = (patch_size_w, patch_size_h), stride = (patch_size_w, patch_size_h)),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
        "        num_patches = (win_len // patch_size_w) * (feature_size // patch_size_h)\n",
        "        print(num_patches)\n",
        "        self.position = nn.Parameter(torch.randn(num_patches + 1, emb_size))\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,1,self.win_len,self.feature_size)\n",
        "        b, _, _, _ = x.shape\n",
        "        # print(x.shape)\n",
        "        x = self.projection(x)\n",
        "        # print(x.shape)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # print(x.shape)\n",
        "        x += self.position\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads = 5, dropout = 0.0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = nn.Linear(emb_size, emb_size*3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "    def forward(self, x, mask = None):\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion = 4, drop_p = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 drop_p = 0.5,\n",
        "                 forward_expansion = 4,\n",
        "                 forward_drop_p = 0.,\n",
        "                 ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 depth = 1,\n",
        "                 **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size, num_classes):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, num_classes))\n",
        "\n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                win_len,\n",
        "                feature_size,\n",
        "                emb_size,\n",
        "                depth = 1,\n",
        "                in_channels = 1,\n",
        "                *,\n",
        "                num_classes,\n",
        "                **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(win_len,\n",
        "                           feature_size,\n",
        "                           emb_size,\n",
        "                           in_channels),\n",
        "            TransformerEncoder(depth,\n",
        "                               emb_size=emb_size,\n",
        "                               **kwargs),\n",
        "            ClassificationHead(emb_size,\n",
        "                               num_classes)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2xEfBs-jCEv"
      },
      "source": [
        "## Transformer Parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB5jnt1_jIwv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, win_len, feature_size, emb_size, in_channels = 1):\n",
        "        self.win_len = win_len\n",
        "        self.feature_size = feature_size\n",
        "        patch_size_w = int(win_len/10)\n",
        "        patch_size_h = int(feature_size/4)\n",
        "        img_size = win_len*feature_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size = (patch_size_w, patch_size_h), stride = (patch_size_w, patch_size_h)),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1,emb_size))\n",
        "        size_ratio = int(img_size/emb_size)\n",
        "        # print(size_ratio)\n",
        "        self.position = nn.Parameter(torch.randn(size_ratio + 1, emb_size))\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,1,self.win_len,self.feature_size)\n",
        "        b, _, _, _ = x.shape\n",
        "        # print(x.shape)\n",
        "        x = self.projection(x)\n",
        "        # print(x.shape)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # print(x.shape)\n",
        "        x += self.position\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads=5, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # print(x.shape)\n",
        "        qkv = self.qkv(x).reshape(x.shape[0], x.shape[1], 3, self.num_heads, self.emb_size // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "        att = F.softmax(energy, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhqk, bhkd -> bhqd', att, values).reshape(x.shape[0], x.shape[1], -1)\n",
        "        return self.projection(out)\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion=4, dropout=0.1):\n",
        "        super(FeedForwardBlock, self).__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads=5, dropout=0.1, forward_expansion=4):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.attention = ResidualAdd(nn.Sequential(\n",
        "            nn.LayerNorm(emb_size),\n",
        "            MultiHeadAttention(emb_size, num_heads, dropout),\n",
        "            nn.Dropout(dropout)\n",
        "        ))\n",
        "        self.feed_forward = ResidualAdd(nn.Sequential(\n",
        "            nn.LayerNorm(emb_size),\n",
        "            FeedForwardBlock(emb_size, forward_expansion, dropout),\n",
        "            nn.Dropout(dropout)\n",
        "        ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention(x)\n",
        "        x = self.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth=1, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(*[\n",
        "            TransformerEncoderBlock(**kwargs) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, win_len, feature_size, emb_size, depth=1, in_channels=1):\n",
        "        super(ViTEncoder, self).__init__()\n",
        "        self.patch_embedding = PatchEmbedding(win_len, feature_size, emb_size, in_channels)\n",
        "        self.encoder = TransformerEncoder(depth=depth, emb_size=emb_size)\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embedding(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.norm(x)\n",
        "        return x.mean(dim=1)  # Pooling the sequence to one vector\n",
        "\n",
        "class ViT_Parallel(nn.Module):\n",
        "    def __init__(self, num_classes, win_len, feature_size, emb_size, depth=1, in_channels=1):\n",
        "        super(ViT_Parallel, self).__init__()\n",
        "        self.encoder = ViTEncoder(win_len, feature_size, emb_size, depth, in_channels)\n",
        "        self.classifier = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x1, x2=None, flag='unsupervised'):\n",
        "        if flag == 'supervised':\n",
        "            x1 = self.encoder(x1)\n",
        "            # x1 = x1.mean(dim=1)  # Aggregate over the sequence length\n",
        "            return self.classifier(x1)\n",
        "        else:\n",
        "            z1 = self.encoder(x1)\n",
        "            if x2 is not None:\n",
        "                z2 = self.encoder(x2)\n",
        "                return z1, z2\n",
        "            return z1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNUpk8fqdkbs"
      },
      "source": [
        "# Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt11wO8idoKp"
      },
      "outputs": [],
      "source": [
        "# class AttenLayer(nn.Module):\n",
        "#     def __init__(self, n_units, input_size=400):\n",
        "#         super(AttenLayer, self).__init__()\n",
        "#         self.kernel = nn.Parameter(torch.Tensor(input_size, n_units))\n",
        "#         self.bias = nn.Parameter(torch.Tensor(n_units))\n",
        "#         self.prob_kernel = nn.Parameter(torch.Tensor(n_units))\n",
        "#         self.reset_parameters()\n",
        "\n",
        "#     def reset_parameters(self):\n",
        "#         nn.init.kaiming_uniform_(self.kernel)\n",
        "#         nn.init.zeros_(self.bias)\n",
        "#         # nn.init.kaiming_uniform_(self.prob_kernel)\n",
        "\n",
        "#     def forward(self, input_tensor):\n",
        "#         # input_tensor = input_tensor.permute(1,0,2)\n",
        "#         batch_size = input_tensor.size()[0]\n",
        "#         # print(input_tensor.shape)\n",
        "#         # print(batch_size)\n",
        "#         # print(self.kernel.size())\n",
        "#         # print(self.bias.size())\n",
        "#         atten_state = torch.tanh(torch.tensordot(input_tensor, self.kernel,dims=1) + self.bias)\n",
        "#         logits = torch.tensordot(atten_state, self.prob_kernel,dims=1)\n",
        "#         prob = nn.functional.softmax(logits, dim=1)\n",
        "#         weighted_feature = torch.sum(torch.mul(input_tensor, torch.unsqueeze(prob, dim=-1)), dim=1)\n",
        "\n",
        "#         return weighted_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdqdcKEkd2Ev"
      },
      "outputs": [],
      "source": [
        "# class BiLSTMAttention(nn.Module):\n",
        "#     def __init__(self, num_classes, n_unit_lstm=200, n_unit_atten=400, downsample=1, win_len=250,feature_input_size=148):\n",
        "#         super(BiLSTMAttention, self).__init__()\n",
        "#         self._downsample = downsample\n",
        "#         self._win_len = win_len\n",
        "#         self._labels = range(num_classes)\n",
        "#         self._feature_input_size = feature_input_size\n",
        "\n",
        "#         # if self._downsample > 1:\n",
        "#         #     length = len(torch.ones((self._win_len,))[::self._downsample])\n",
        "#         #     self.x_in = nn.Linear(length,148)\n",
        "#         # else:\n",
        "#         #     self.x_in = nn.Linear(self._win_len,148)\n",
        "\n",
        "#         self.lstm = nn.LSTM(feature_input_size, hidden_size=n_unit_lstm, bidirectional=True, batch_first=True)\n",
        "#         self.atten = AttenLayer(n_unit_atten, input_size=400)\n",
        "#         self.dense = nn.Linear(n_unit_atten, num_classes)\n",
        "#         self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_tensor = x.view(-1,self._win_len, self._feature_input_size)\n",
        "#         # x_I = x[0]\n",
        "#         # plt.imshow(x_I)\n",
        "#         # plt.show()\n",
        "#         # x_tensor = self.x_in(x)\n",
        "#         # print(x_tensor.shape)\n",
        "#         # x_tensor = x_tensor.permute(0, 2, 1)  # Transpose to (batch_size, sequence_length, features)\n",
        "#         x_tensor, _ = self.lstm(x_tensor) #output(batch_size,sequence_length,2*n_unit_lstm)\n",
        "#         # x_tensor = x_tensor.permute(0, 2, 1)\n",
        "#         x_tensor = self.atten(x_tensor)\n",
        "#         x_tensor = self.dense(x_tensor)\n",
        "#         pred = self.softmax(x_tensor)\n",
        "#         return x_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns2hUDV8dwpf"
      },
      "source": [
        "#BiLSTM+Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRh4bkReXvXH"
      },
      "outputs": [],
      "source": [
        "# class BiLSTM_Attention(nn.Module):\n",
        "#     def __init__(self,n_hidden,embedding_dim,num_classes):\n",
        "#         super(BiLSTM_Attention, self).__init__()\n",
        "#         self.embedding_dim = embedding_dim\n",
        "#         self.n_hidden=n_hidden\n",
        "#         self.num_classes = num_classes\n",
        "#         # self.embedding = nn.Embedding(vocab_size, embedding_dim)#embedding_dim = 148\n",
        "#         self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
        "#         self.out = nn.Linear(n_hidden * 2, num_classes)\n",
        "\n",
        "#     # lstm_output : [batch_size, n_step, n_hidden * num_directions(=2)], F matrix\n",
        "#     def attention_net(self, lstm_output, final_state):\n",
        "#         hidden = final_state.view(-1, self.n_hidden * 2, 1)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
        "#         attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
        "#         soft_attn_weights = F.softmax(attn_weights, 1)\n",
        "#         # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
        "#         context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
        "#         return context#, soft_attn_weights.data.numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         input = x.view(-1,148,300) # input : [batch_size, len_seq, embedding_dim](1,300,148)\n",
        "#         input = input.permute(2, 0, 1) # input : [len_seq, batch_size, embedding_dim](300,batch,148)\n",
        "\n",
        "#         hidden_state = Variable(torch.zeros(1*2, len(x), self.n_hidden)).to(device) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
        "#         cell_state = Variable(torch.zeros(1*2, len(x), self.n_hidden)).to(device) # [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
        "\n",
        "#         # final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\n",
        "#         output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
        "#         output = output.permute(1, 0, 2) # output : [batch_size, len_seq, n_hidden]\n",
        "#         attn_output = self.attention_net(output, final_hidden_state)\n",
        "#         context = self.out(attn_output)\n",
        "#         # context = F.softmax(context, dim=1)\n",
        "#         return context #, attention # model : [batch_size, num_classes], attention : [batch_size, n_step]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMN5PXcixZRh"
      },
      "source": [
        "# Organization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZl_22krMTSn"
      },
      "source": [
        "## Supervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqCjYEkz0NCB"
      },
      "outputs": [],
      "source": [
        "def load_data(task, experiment, data_dir,batch_size,test_data):\n",
        "  # get the class number of different task\n",
        "  classes = {'HumanNonhuman': 2, 'FourClass': 4, 'HumanID': 4, 'HumanMotion': 3, 'ThreeClass': 3}\n",
        "\n",
        "  # retrive the data\n",
        "  train_set = OW_dataset_class(data_dir, task, experiment, 0, test_data)\n",
        "  num_human = train_set.labels.count(1)\n",
        "  print(len(train_set.labels))\n",
        "  print(train_set.samples.shape)\n",
        "  # Split dataset into train and validation sets\n",
        "  train_size = int(0.5 * len(train_set))\n",
        "  valid_size = len(train_set) - train_size\n",
        "  train_set, valid_set = random_split(train_set, [train_size, valid_size])\n",
        "\n",
        "  # Create data loaders for train and validation sets\n",
        "  train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "  valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "  test_set = OW_dataset_class(data_dir, task, experiment, 1, test_data=test_data)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "  ssl_set = OW_dataset_class(data_dir, task, experiment, 1, test_data=2)\n",
        "  ssl_loader = torch.utils.data.DataLoader(ssl_set, batch_size=32, shuffle=False)\n",
        "\n",
        "  return train_loader, valid_loader, test_loader,ssl_loader\n",
        "\n",
        "def load_model(task, model_name, win_len,feature_size):\n",
        "  classes = {'HumanNonhuman':2, 'FourClass':4, 'HumanID':4, 'HumanMotion':3, 'ThreeClass':3}\n",
        "  # Based on different model, adjust the setting\n",
        "  if model_name == 'MLP':\n",
        "    print(\"using model: MLP\")\n",
        "    model = OW_MLP(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'LeNet':\n",
        "    print(\"using model: LeNet\")\n",
        "    model = OW_LeNet(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'ResNet18':\n",
        "    print(\"using model: ResNet18\")\n",
        "    model = ResNet18(classes[task], win_len, feature_size)\n",
        "    print(\"Model after initialization:\", model)  # add this line\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'ResNet50':\n",
        "    print(\"using model: ResNet50\")\n",
        "    model = ResNet50(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'ResNet101':\n",
        "    print(\"using model: ResNet101\")\n",
        "    model = ResNet101(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'GRUNet':\n",
        "    print(\"using model: GRUNet\")\n",
        "    model = GRUNet(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'RNN':\n",
        "    print(\"using model: RNN\")\n",
        "    model = RNN(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'LSTM':\n",
        "    print(\"using model: LSTM\")\n",
        "    model = LSTM(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'BiLSTM':\n",
        "    print(\"using model: BiLSTM\")\n",
        "    model = BiLSTM(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'CNN+GRU':\n",
        "    print(\"using model: CNN+GRU\")\n",
        "    model = CNN_GRU(classes[task], win_len, feature_size)\n",
        "    train_epoch = 100\n",
        "\n",
        "  elif model_name == 'ViT':\n",
        "    print(\"using model: ViT\")\n",
        "    emb_size = int((win_len / 10) * (feature_size / 4))\n",
        "    print(emb_size)\n",
        "    model = ViT(win_len, feature_size, emb_size, num_classes=classes[task])\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'BiLSTMAttention':\n",
        "    print(\"using model: BiLSTMAttention\")\n",
        "\n",
        "    model = BiLSTMAttention(classes[task], win_len, feature_size, n_unit_lstm=200, n_unit_atten=400)\n",
        "    train_epoch = 200\n",
        "  else:\n",
        "    print(\"Model Unavaliable!\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtcZQcq7MX3H"
      },
      "source": [
        "## unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-CeqYHlFkNl"
      },
      "outputs": [],
      "source": [
        "def load_unsupervised_model(task, model_name, win_len,feature_size,depth=1):\n",
        "\n",
        "  classes = {'HumanNonhuman':2, 'FourClass':4, 'HumanID':4, 'HumanMotion':3, 'ThreeClass':3}\n",
        "\n",
        "  # Based on different model, adjust the setting\n",
        "  if model_name == 'MLP':\n",
        "    print(\"using model: MLP_Parrallel\")\n",
        "    model = MLP_Parrallel(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'LeNet':\n",
        "    print(\"using model: CNN_Parrallel\")\n",
        "    model = CNN_Parrallel(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'ResNet18':\n",
        "    print(\"using model: ResNet18_Parrallel\")\n",
        "    model = ResNet18_Parrallel(classes[task])\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'ResNet50':\n",
        "    print(\"using model: ResNet50_Parrallel\")\n",
        "    model = ResNet50_Parrallel(classes[task])\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'ResNet101':\n",
        "    print(\"using model: ResNet101_Parrallel\")\n",
        "    model = ResNet101_Parrallel(classes[task])\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'GRU':\n",
        "    print(\"using model: GRU_Parrallel\")\n",
        "    model = GRU_Parrallel(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'RNN':\n",
        "    print(\"using model: RNN_Parrallel\")\n",
        "    model = RNN_Parrallel(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "  elif model_name == 'LSTM':\n",
        "    print(\"using model: LSTM_Parrallel\")\n",
        "    model = LSTM_Parrallel(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'BiLSTM':\n",
        "    print(\"using model: BiLSTM_Parrallel\")\n",
        "    model = BiLSTM_Parrallel(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'CNN+GRU':\n",
        "    print(\"using model: CNN_GRU_Parrallel\")\n",
        "    model = CNN_GRU_Parrallel(classes[task], win_len, feature_size)\n",
        "    train_epoch = 200\n",
        "\n",
        "  elif model_name == 'ViT':\n",
        "    print(\"using model: ViT_Parrallel\")\n",
        "    emb_size = int((win_len / 10) * (feature_size / 4))\n",
        "    print(emb_size)\n",
        "    model = ViT_Parallel(classes[task], win_len, feature_size,emb_size,depth=depth)\n",
        "    train_epoch = 200\n",
        "\n",
        "  else:\n",
        "    print(\"Model Unavaliable!\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhAe1cAavMsO"
      },
      "source": [
        "# Train & Test Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxiPjJBryLrx"
      },
      "outputs": [],
      "source": [
        "def checkpoint(model, filename):\n",
        "    torch.save(model.state_dict(), filename)\n",
        "\n",
        "def resume(model, filename):\n",
        "    model.load_state_dict(torch.load(filename))\n",
        "\n",
        "def warmup_schedule(epoch, warmup_epochs):\n",
        "    if epoch < warmup_epochs:\n",
        "        return epoch / warmup_epochs\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "def save_checkpoint(state, filename=\"model_checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(filepath, model):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    return model, epoch, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B589Fh-V-VY"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDBxXt5shQBF"
      },
      "outputs": [],
      "source": [
        "# def test(model, test_dataloader,task,save_path):\n",
        "\n",
        "#     # constant for classes\n",
        "#     if task == 'FourClass':\n",
        "#       classes = ('Human', 'Pet', 'IRobot', 'Fan')\n",
        "#       labels = np.array([0, 1, 2, 3])  # assuming the classes are coded as 0,1,2,3\n",
        "#     elif task == 'HumanNonhuman':\n",
        "#       classes = ('Human','Nonhuman')\n",
        "#       labels = np.array([0, 1])  # assuming the classes are coded as 0,1\n",
        "#     elif task == 'ThreeClass':\n",
        "#       classes = ('Human', 'Pet', 'IRobot')\n",
        "#       labels = np.array([0, 1, 2])  # assuming the classes are coded as 0,1,2\n",
        "\n",
        "\n",
        "#     model.eval()\n",
        "#     correct_1, correct_2 = 0, 0\n",
        "#     total = 0\n",
        "\n",
        "#     y_pred_1 = []\n",
        "#     y_pred_2 = []\n",
        "#     y_true = []\n",
        "#     t= []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for data in test_dataloader:\n",
        "#             x, y = data\n",
        "#             x, y = x.to(device), y.to(device)\n",
        "\n",
        "#             y1, y2 = model(x, x, flag='supervised')\n",
        "#             _, pred_1 = torch.max(y1.data, 1)\n",
        "#             _, pred_2 = torch.max(y2.data, 1)\n",
        "#             total += y.size(0)\n",
        "#             correct_1 += (pred_1 == y).sum().item()\n",
        "#             correct_2 += (pred_2 == y).sum().item()\n",
        "\n",
        "#             y_pred_1.extend(pred_1)\n",
        "#             y_pred_2.extend(pred_2)\n",
        "#             y_true.extend(y)\n",
        "\n",
        "#     y_true = torch.Tensor(y_true)\n",
        "#     y_pred_1 = torch.Tensor(y_pred_1)\n",
        "#     y_pred_2 = torch.Tensor(y_pred_2)\n",
        "\n",
        "#     print('Test accuracy: {:.2f}%, {:.2f}%'.format(100 * correct_1 / total, 100 * correct_2 / total))\n",
        "\n",
        "#     # plot the first result------------------------------------------\n",
        "#     cf_matrix = confusion_matrix(y_true, y_pred_1, labels=labels)\n",
        "#     row_sums = np.sum(cf_matrix, axis=1)[:, None]\n",
        "#     normalized_cf_matrix = np.where(row_sums != 0, cf_matrix / row_sums, 0)\n",
        "\n",
        "#     df_cm = pd.DataFrame(normalized_cf_matrix, index = [i for i in classes],\n",
        "#                         columns = [i for i in classes])\n",
        "#     fig_cm, ax1 = plt.subplots(figsize = (12,7))\n",
        "#     sn.heatmap(df_cm, annot=True, ax = ax1)\n",
        "#     ax1.set_title('Normalized confusion matrix')\n",
        "#     ax1.set_xlabel('Predicted classes')\n",
        "#     ax1.set_ylabel('Actual classes')\n",
        "#     fig_cm.savefig(save_path+\"_test_confusionmat_normalized_1.png\")\n",
        "#     df_cm.to_csv(save_path+\"_test_confusionmat_normalized_1.csv\")\n",
        "\n",
        "#     df_cm_un = pd.DataFrame(cf_matrix, index = [i for i in classes],\n",
        "#                         columns = [i for i in classes])\n",
        "#     fig_cm_un, ax2 = plt.subplots(figsize = (12,7))\n",
        "#     sn.heatmap(df_cm_un, annot=True, ax = ax2)\n",
        "#     ax2.set_title('Unnormalized confusion matrix')\n",
        "#     ax2.set_xlabel('Prediected classes')\n",
        "#     ax2.set_ylabel('Actual classes')\n",
        "#     fig_cm_un.savefig(save_path+\"_test_confusionmat_unnormalized_1.png\")\n",
        "#     df_cm.to_csv(save_path+\"_test_confusionmat_unnormalized_1.csv\")\n",
        "\n",
        "#     pd_test_report = pd.DataFrame(classification_report(y_true, y_pred_1, output_dict=True)).transpose()\n",
        "#     pd_test_report.to_csv(save_path+\"_test_report_1.csv\")\n",
        "\n",
        "#     test_results = {'Test Accuracy': float(correct_1) / total }\n",
        "#     t.append(test_results)\n",
        "\n",
        "#     # plot the second result------------------------------------------\n",
        "#     cf_matrix = confusion_matrix(y_true, y_pred_2, labels=labels)\n",
        "#     row_sums = np.sum(cf_matrix, axis=1)[:, None]\n",
        "#     normalized_cf_matrix = np.where(row_sums != 0, cf_matrix / row_sums, 0)\n",
        "\n",
        "#     df_cm = pd.DataFrame(normalized_cf_matrix, index = [i for i in classes],\n",
        "#                         columns = [i for i in classes])\n",
        "#     fig_cm, ax1 = plt.subplots(figsize = (12,7))\n",
        "#     sn.heatmap(df_cm, annot=True, ax = ax1)\n",
        "#     ax1.set_title('Normalized confusion matrix')\n",
        "#     ax1.set_xlabel('Predicted classes')\n",
        "#     ax1.set_ylabel('Actual classes')\n",
        "#     fig_cm.savefig(save_path+\"_test_confusionmat_normalized_2.png\")\n",
        "#     df_cm.to_csv(save_path+\"_test_confusionmat_normalized_2.csv\")\n",
        "\n",
        "#     df_cm_un = pd.DataFrame(cf_matrix, index = [i for i in classes],\n",
        "#                         columns = [i for i in classes])\n",
        "#     fig_cm_un, ax2 = plt.subplots(figsize = (12,7))\n",
        "#     sn.heatmap(df_cm_un, annot=True, ax = ax2)\n",
        "#     ax2.set_title('Unnormalized confusion matrix')\n",
        "#     ax2.set_xlabel('Prediected classes')\n",
        "#     ax2.set_ylabel('Actual classes')\n",
        "#     fig_cm_un.savefig(save_path+\"_test_confusionmat_unnormalized_2.png\")\n",
        "#     df_cm.to_csv(save_path+\"_test_confusionmat_unnormalized_2.csv\")\n",
        "\n",
        "#     pd_test_report = pd.DataFrame(classification_report(y_true, y_pred_2, output_dict=True)).transpose()\n",
        "#     pd_test_report.to_csv(save_path+\"_test_report_2.csv\")\n",
        "\n",
        "#     test_results = {'Test Accuracy': float(correct_2) / total }\n",
        "#     t.append(test_results)\n",
        "\n",
        "#     df_test = pd.DataFrame.from_records(t)\n",
        "\n",
        "#     df_test.to_csv(save_path+\"result.csv\")\n",
        "#     return df_test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYyGX-5Pnb73"
      },
      "outputs": [],
      "source": [
        "def test(model, test_dataloader, task, save_path):\n",
        "    # Constants for classes based on the task\n",
        "    class_maps = {\n",
        "        'FourClass': ('Human', 'Pet', 'IRobot', 'Fan'),\n",
        "        'HumanNonhuman': ('Human', 'Nonhuman'),\n",
        "        'ThreeClass': ('Human', 'Pet', 'IRobot')\n",
        "    }\n",
        "    classes = class_maps[task]\n",
        "    labels = np.arange(len(classes))  # Auto-generate labels based on class count\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_dataloader:\n",
        "            x, y = data\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            outputs = model(x, flag='supervised')\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "\n",
        "    # Accuracy\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "    # Confusion Matrix and Classification Report\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    df_cm = pd.DataFrame(cf_matrix, index=classes, columns=classes)\n",
        "    sn.heatmap(df_cm, annot=True)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Classes')\n",
        "    plt.ylabel('Actual Classes')\n",
        "    plt.savefig(f\"{save_path}_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "    report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
        "    df_report = pd.DataFrame(report).transpose()\n",
        "    df_report.to_csv(f\"{save_path}_classification_report.csv\")\n",
        "\n",
        "    return df_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prFm0HoKWq9U"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOOsQLG4WtVR"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, RandomApply, ColorJitter, RandomGrayscale, GaussianBlur, ToTensor, Normalize\n",
        "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class TensorToPILTransform:\n",
        "    \"\"\"Convert a tensor to a PIL Image.\"\"\"\n",
        "    def __call__(self, tensor):\n",
        "        return to_pil_image(tensor)\n",
        "\n",
        "def get_simclr_augmentation(image_size):\n",
        "    \"\"\"Get a set of data augmentation transformations as described for SimCLR.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=image_size, scale=(0.2, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)  # randomly change the brightness, contrast and saturation\n",
        "        ], p=0.8),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.GaussianBlur(kernel_size=int(0.1 * image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # standard normalization\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GaussianNoiseTransform:\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        # Generate noise on the same device as the input tensor\n",
        "        noise = torch.randn(tensor.size(), device=tensor.device) * self.std + self.mean\n",
        "        return tensor + noise\n",
        "\n",
        "class RandomScaleTransform:\n",
        "    def __init__(self, scale_limit=(0.9, 1.1)):\n",
        "        self.scale_limit = scale_limit\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        scale = torch.empty(1, device=tensor.device).uniform_(*self.scale_limit)\n",
        "        return tensor * scale\n",
        "\n",
        "class RandomTimeShiftTransform:\n",
        "    def __init__(self, shift_limit):\n",
        "        self.shift_limit = shift_limit\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        shift = torch.randint(low=-self.shift_limit, high=self.shift_limit, size=(1,), device=tensor.device)\n",
        "        return torch.roll(tensor, shifts=shift.item(), dims=1)\n",
        "\n",
        "class JitterTransform:\n",
        "    def __init__(self, jitter_factor=0.05):\n",
        "        self.jitter_factor = jitter_factor\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        jitter = torch.randn(tensor.size(), device=tensor.device) * self.jitter_factor\n",
        "        return tensor + jitter\n",
        "\n",
        "class RandomCropTransform:\n",
        "    def __init__(self, crop_size):\n",
        "        self.crop_size = crop_size\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        start = torch.randint(0, tensor.size(1) - self.crop_size + 1, size=(1,), device=tensor.device)\n",
        "        return tensor[:, start:start + self.crop_size]\n",
        "\n",
        "\n",
        "def get_augmentation(image_size):\n",
        "    \"\"\"Combines SimCLR and custom augmentations tailored for tensors.\"\"\"\n",
        "    return Compose([\n",
        "        # Convert tensor to PIL to use certain transformations\n",
        "        TensorToPILTransform(),\n",
        "\n",
        "        # SimCLR transformations\n",
        "        # RandomResizedCrop(size=image_size, scale=(0.2, 1.0)),\n",
        "        RandomHorizontalFlip(),\n",
        "        RandomApply([ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "        RandomGrayscale(p=0.2),\n",
        "        # GaussianBlur(kernel_size=int(0.1 * image_size)),\n",
        "\n",
        "        # Convert back to tensor\n",
        "        ToTensor(),\n",
        "\n",
        "        # Custom transformations that work on tensors\n",
        "        RandomApply([GaussianNoiseTransform(std=0.1)], p=0.5),\n",
        "        Normalize(mean=[0.485], std=[0.229])  # Example values for grayscale images\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjL0DXALWB3Z"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlGlhdENnJBr"
      },
      "outputs": [],
      "source": [
        "def train(model, unsupervised_train_loader, supervised_train_loader, supervised_val_loader, test_dataloader, num_epochs_1, num_epochs_2, learning_rate, criterion, ce_criterion, device, patience,save_path):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1.5e-6)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)  # Adjust step size and gamma as needed\n",
        "\n",
        "    # Records for tracking progress\n",
        "    unsupervised_records = []\n",
        "    supervised_records = []\n",
        "\n",
        "    # Define the image size for augmentations\n",
        "    image_size = 224  # Adjust as needed for your model\n",
        "    augmentation = get_augmentation(image_size)\n",
        "\n",
        "    train_losses = []\n",
        "\n",
        "    print('Starting self-supervised training phase.')\n",
        "    for epoch in range(num_epochs_1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data in unsupervised_train_loader:\n",
        "            x, _ = data  # Ignore labels\n",
        "            x = x.to(device)\n",
        "\n",
        "            # Apply augmentation to each image in the batch to get x1, x2\n",
        "            x1 = torch.stack([augmentation(img) for img in x]).to(device)\n",
        "            x2 = torch.stack([augmentation(img) for img in x]).to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            z1 = model(x1)\n",
        "            z2 = model(x2)\n",
        "            z1 = F.normalize(z1, p=2, dim=1)\n",
        "            z2 = F.normalize(z2, p=2, dim=1)\n",
        "\n",
        "            # Calculate contrastive loss\n",
        "            loss = nt_xent_loss(z1, z2)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(unsupervised_train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs_1}, Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "    # Save the final model checkpoint after self-supervised training\n",
        "    save_checkpoint({\n",
        "        'epoch': num_epochs_1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_train_loss,\n",
        "    }, filename=save_path+\"self_supervised_model_checkpoint.pth.tar\")\n",
        "\n",
        "    # After training, plot the training loss\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    epochs = range(1, num_epochs_1 + 1)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.lineplot(x=epochs, y=train_losses, label='Training Loss', ax=ax)\n",
        "    ax.set_title('Training Loss')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the figure\n",
        "    fig.savefig(save_path + \"train_loss.png\")\n",
        "\n",
        "    # Supervised training phase\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print('Starting supervised training phase.')\n",
        "    for epoch in range(num_epochs_2):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for data in supervised_train_loader:\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(x, flag='supervised')\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(supervised_train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data in supervised_val_loader:\n",
        "                x, y = data\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                # Forward pass through the model\n",
        "                outputs = model(x, flag='supervised')\n",
        "                loss = criterion(outputs, y)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(supervised_val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs_2}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Checkpointing\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_val_loss,\n",
        "            }, filename=f\"{save_path}_supervised_model_checkpoint_epoch_{epoch+1}.pth.tar\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Plotting training and validation loss\n",
        "    epochs = range(1, num_epochs_2 + 1)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.lineplot(x=epochs, y=train_losses, label='Training Loss', ax=ax)\n",
        "    sns.lineplot(x=epochs, y=val_losses, label='Validation Loss', ax=ax)\n",
        "    ax.set_title('Training and Validation Loss')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the figure\n",
        "    fig.savefig(save_path + \"supervised_train_validation_loss.png\")\n",
        "\n",
        "\n",
        "    return model, unsupervised_records, supervised_records\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unsupervised train"
      ],
      "metadata": {
        "id": "rftvuIfa2fcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, LambdaLR\n",
        "\n",
        "def unsupervised_train(model, unsupervised_train_loader, num_epochs_1, learning_rate, criterion, device, patience, save_path):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-8)\n",
        "\n",
        "    # Implementing SGDR with Cosine Annealing\n",
        "    T_0 = 10  # Initial number of epochs in the first cycle before restart\n",
        "    scheduler_cosine = CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=2, eta_min=0.0001)\n",
        "\n",
        "    # Implementing Learning Rate Warmup\n",
        "    warmup_epochs = 5\n",
        "    scheduler_warmup = LambdaLR(optimizer, lr_lambda=lambda epoch: min((epoch + 1) / warmup_epochs, 1))\n",
        "\n",
        "    # Early stopping and model checkpointing\n",
        "    best_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # Records for tracking progress\n",
        "    unsupervised_records = []\n",
        "    train_losses = []\n",
        "    learning_rates = []\n",
        "    # Define the image size for augmentations\n",
        "    image_size = 224  # Adjust as needed for your model\n",
        "    augmentation = get_augmentation(image_size)\n",
        "\n",
        "\n",
        "    print('Starting self-supervised training phase.')\n",
        "    for epoch in range(num_epochs_1):\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        learning_rates.append(current_lr)\n",
        "        print(f'Epoch {epoch+1}, Current LR: {current_lr}')\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data in unsupervised_train_loader:\n",
        "            x, _ = data  # Ignore labels\n",
        "            x = x.to(device)\n",
        "\n",
        "            # Apply augmentation to each image in the batch to get x1, x2\n",
        "            x1 = torch.stack([augmentation(img) for img in x]).to(device)\n",
        "            x2 = torch.stack([augmentation(img) for img in x]).to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            z1 = model(x1)\n",
        "            z2 = model(x2)\n",
        "            z1 = F.normalize(z1, p=2, dim=1)\n",
        "            z2 = F.normalize(z2, p=2, dim=1)\n",
        "\n",
        "            # Calculate contrastive loss\n",
        "            loss = nt_xent_loss(z1, z2)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(unsupervised_train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs_1}, Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        # Update scheduler after each epoch\n",
        "        if epoch < warmup_epochs:\n",
        "            scheduler_warmup.step()  # Warmup phase\n",
        "        else:\n",
        "            scheduler_cosine.step(epoch - warmup_epochs)  # Adjust based on the total epochs minus warmup epochs\n",
        "\n",
        "        # Check for early stopping\n",
        "        if avg_train_loss < best_loss:\n",
        "            best_loss = avg_train_loss\n",
        "            epochs_no_improve = 0\n",
        "            # Save the best model\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_train_loss,\n",
        "            }, filename=save_path + \"best_model_checkpoint_ssl.pth.tar\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f'Early stopping triggered after {patience} epochs without improvement')\n",
        "            break\n",
        "\n",
        "    # After training, plot the training loss\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.lineplot(x=epochs, y=train_losses, label='Training Loss', ax=ax)\n",
        "    ax.set_title('Training Loss')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the figure\n",
        "    fig.savefig(save_path + \"train_loss.png\")\n",
        "\n",
        "    # Plotting learning rate progression\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(range(1, num_epochs_1 + 1), learning_rates, marker='o', linestyle='-')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate Progression')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return model, unsupervised_records\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0T3N1IoNI1fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27n3Bf_NjmdY"
      },
      "source": [
        "## Supervised Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcOaKKb0dSkj"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def train_supervised(model, supervised_train_loader, supervised_val_loader, test_dataloader, num_epochs_2, learning_rate, ce_criterion, device, optimizer_decay_rate, patience,save_path):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=optimizer_decay_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)  # Adjust step size and gamma as needed\n",
        "\n",
        "    # Records for tracking progress\n",
        "    supervised_records = []\n",
        "\n",
        "    # Define the image size for augmentations\n",
        "    image_size = 224  # Adjust as needed for your model\n",
        "    augmentation = get_augmentation(image_size)\n",
        "    # Supervised training phase\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    print('Starting supervised training phase.')\n",
        "    for epoch in range(num_epochs_2):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        for data in supervised_train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(x, flag='supervised')\n",
        "            loss = ce_criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(supervised_train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data in supervised_val_loader:\n",
        "                x, y = data\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "\n",
        "                # Forward pass through the model\n",
        "                outputs = model(x, flag='supervised')\n",
        "                loss = ce_criterion(outputs, y)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(supervised_val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs_2}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Checkpointing\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            save_checkpoint({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_val_loss,\n",
        "            }, filename=f\"{save_path}_supervised_model_checkpoint_epoch_{epoch+1}.pth.tar\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Plotting training and validation loss\n",
        "    epochs = range(1, num_epochs_2 + 1)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.lineplot(x=epochs, y=train_losses, label='Training Loss', ax=ax)\n",
        "    sns.lineplot(x=epochs, y=val_losses, label='Validation Loss', ax=ax)\n",
        "    ax.set_title('Training and Validation Loss')\n",
        "    ax.set_xlabel('Epochs')\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the figure\n",
        "    fig.savefig(save_path + \"supervised_train_validation_loss.png\")\n",
        "\n",
        "\n",
        "    return model, supervised_records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq_r1gAZnUVX"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRl_zokVnIv-"
      },
      "outputs": [],
      "source": [
        "class EntLoss(nn.Module):\n",
        "    def __init__(self, args, lam1, lam2, pqueue=None):\n",
        "        super(EntLoss, self).__init__()\n",
        "        self.lam1 = lam1\n",
        "        self.lam2 = lam2\n",
        "        self.pqueue = pqueue\n",
        "        self.args = args\n",
        "\n",
        "    def forward(self, feat1, feat2, use_queue=False):\n",
        "        probs1 = torch.nn.functional.softmax(feat1, dim=-1)\n",
        "        probs2 = torch.nn.functional.softmax(feat2, dim=-1)\n",
        "        loss = dict()\n",
        "        loss['kl'] = 0.5 * (KL(probs1, probs2, self.args) + KL(probs2, probs1, self.args))\n",
        "\n",
        "        sharpened_probs1 = torch.nn.functional.softmax(feat1/self.args.tau, dim=-1)\n",
        "        sharpened_probs2 = torch.nn.functional.softmax(feat2/self.args.tau, dim=-1)\n",
        "        loss['eh'] = 0.5 * (EH(sharpened_probs1, self.args) + EH(sharpened_probs2, self.args))\n",
        "\n",
        "        # whether use historical data\n",
        "        loss['he'] = 0.5 * (HE(sharpened_probs1, self.args) + HE(sharpened_probs2, self.args))\n",
        "\n",
        "        # TWIST Loss\n",
        "        loss['final'] = loss['kl'] + ((1+self.lam1)*loss['eh'] - self.lam2*loss['he'])\n",
        "\n",
        "        #########################################################################\n",
        "        # probability distribution (PKT by Kernel Density Estimation)\n",
        "        loss['kde'] = cosine_similarity_loss(feat1, feat2)\n",
        "\n",
        "        # nuclear-norm\n",
        "        loss['n-norm'] = -0.5 * (torch.norm(sharpened_probs1,'nuc')+torch.norm(sharpened_probs2,'nuc')) * 0.001\n",
        "\n",
        "        loss['final-kde'] = loss['kde'] * 100 + loss['final']#+ loss['n-norm']\n",
        "\n",
        "        return loss\n",
        "\n",
        "def KL(probs1, probs2, args):\n",
        "    kl = (probs1 * (probs1 + args.EPS).log() - probs1 * (probs2 + args.EPS).log()).sum(dim=1)\n",
        "    kl = kl.mean()\n",
        "    return kl\n",
        "\n",
        "def CE(probs1, probs2, args):\n",
        "    ce = - (probs1 * (probs2 + args.EPS).log()).sum(dim=1)\n",
        "    ce = ce.mean()\n",
        "    return ce\n",
        "\n",
        "def HE(probs, args):\n",
        "    mean = probs.mean(dim=0)\n",
        "    ent  = - (mean * (mean + args.EPS).log()).sum()\n",
        "    return ent\n",
        "\n",
        "def EH(probs, args):\n",
        "    ent = - (probs * (probs + args.EPS).log()).sum(dim=1)\n",
        "    mean = ent.mean()\n",
        "    return mean\n",
        "\n",
        "def cosine_similarity_loss(output_net, target_net, eps=0.0000001):\n",
        "    # Normalize each vector by its norm\n",
        "    output_net_norm = torch.sqrt(torch.sum(output_net ** 2, dim=1, keepdim=True))\n",
        "    output_net = output_net / (output_net_norm + eps)\n",
        "    output_net[output_net != output_net] = 0\n",
        "\n",
        "    target_net_norm = torch.sqrt(torch.sum(target_net ** 2, dim=1, keepdim=True))\n",
        "    target_net = target_net / (target_net_norm + eps)\n",
        "    target_net[target_net != target_net] = 0\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    model_similarity = torch.mm(output_net, output_net.transpose(0, 1))\n",
        "    target_similarity = torch.mm(target_net, target_net.transpose(0, 1))\n",
        "\n",
        "    # Scale cosine similarity to 0..1\n",
        "    model_similarity = (model_similarity + 1.0) / 2.0\n",
        "    target_similarity = (target_similarity + 1.0) / 2.0\n",
        "\n",
        "    # Transform them into probabilities\n",
        "    model_similarity = model_similarity / torch.sum(model_similarity, dim=1, keepdim=True)\n",
        "    target_similarity = target_similarity / torch.sum(target_similarity, dim=1, keepdim=True)\n",
        "\n",
        "    # Calculate the KL-divergence\n",
        "    loss = torch.mean(target_similarity * torch.log((target_similarity + eps) / (model_similarity + eps)))\n",
        "\n",
        "    return loss\n",
        "\n",
        "def gaussian_noise(std,csi, epsilon,win_len,feature_size):\n",
        "    noise = torch.normal(0, std, size=(1,win_len,feature_size)).cuda()\n",
        "    perturbed_csi = csi + epsilon*noise\n",
        "    return perturbed_csi\n",
        "\n",
        "\n",
        "\n",
        "class InfoNCELoss(nn.Module):\n",
        "    def __init__(self, temperature=1.0):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        # z1 = F.normalize(z1, p=2, dim=1)\n",
        "        # z2 = F.normalize(z2, p=2, dim=1)\n",
        "        features = torch.cat([z1, z2], dim=0)\n",
        "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
        "        # # Debugging outputs\n",
        "        # print(\"Average dot product:\", torch.mean(sim_matrix).item())\n",
        "        # print(\"Max dot product:\", torch.max(sim_matrix).item())\n",
        "        # print(\"Min dot product:\", torch.min(sim_matrix).item())\n",
        "\n",
        "        labels = torch.cat([torch.arange(z1.size(0)) for _ in range(2)], dim=0).to(features.device)\n",
        "        sim_matrix.fill_diagonal_(-1e9)\n",
        "        loss = F.cross_entropy(sim_matrix, labels)\n",
        "        return loss\n",
        "\n",
        "class TripletLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = F.pairwise_distance(anchor, positive)\n",
        "        distance_negative = F.pairwise_distance(anchor, negative)\n",
        "        loss = torch.mean(torch.relu(distance_positive - distance_negative + self.margin))\n",
        "        return loss\n",
        "\n",
        "def nt_xent_loss(z1, z2, temperature=0.5):\n",
        "    \"\"\"\n",
        "    Normalized Temperature-scaled Cross Entropy Loss as used in SimCLR\n",
        "    \"\"\"\n",
        "    batch_size = z1.shape[0]\n",
        "    z = torch.cat([z1, z2], dim=0)  # Concatenate the positive pairs\n",
        "    sim_matrix = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2) / temperature\n",
        "\n",
        "    # Create labels for the positive pairs across 2N elements\n",
        "    labels = torch.range(batch_size, 2*batch_size-1).long().to(z.device)\n",
        "    labels = torch.cat([labels, torch.range(0, batch_size-1).long().to(z.device)])\n",
        "\n",
        "    # Calculate cross entropy loss\n",
        "    loss = F.cross_entropy(sim_matrix, labels)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLrODd-bnwlW"
      },
      "source": [
        "# Start Training/Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNB12CuGFz46"
      },
      "source": [
        "### load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJDPcU8bdr4h"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser('Self-Supervised')\n",
        "parser.add_argument('--tau', type=float, default=0.75, metavar='LR')\n",
        "parser.add_argument('--EPS', type=float, default=1e-5, help='episillon')\n",
        "parser.add_argument('--weight-decay', type=float, default=1.5e-6, help='weight decay (default: 1e-4)')\n",
        "parser.add_argument('--lam1', type=float, default=0.0, metavar='LR')\n",
        "parser.add_argument('--lam2', type=float, default=1.0, metavar='LR')\n",
        "parser.add_argument('--local_crops_number', type=int, default=12)\n",
        "parser.add_argument('--min1', type=float, default=0.4, metavar='LR')\n",
        "parser.add_argument('--max1', type=float, default=1.0, metavar='LR')\n",
        "parser.add_argument('--min2', type=float, default=0.05, metavar='LR')\n",
        "parser.add_argument('--max2', type=float, default=0.4, metavar='LR')\n",
        "parser.add_argument('--gpu', type=int, default=1, metavar='gpu')\n",
        "parser.add_argument('--eval', type=str, default='no', metavar='gpu')\n",
        "parser.add_argument('--model', choices = ['MLP','LeNet','ResNet18','ResNet50','ResNet101','RNN','GRU','LSTM','BiLSTM','CNN+GRU','ViT'])\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "# #Decide the model want to train\n",
        "# args.model = 'ResNet18'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLV7yl1ivOS3",
        "outputId": "3560ff9b-d216-4f64-ee3a-2f4318685a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IRobot labeled\n",
            "Pet labeled\n",
            "Fan labeled\n",
            "3\n",
            "Human labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "IRobot labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Fan labeled\n",
            "3\n",
            "Fan labeled\n",
            "3\n",
            "Fan labeled\n",
            "3\n",
            "26791\n",
            "torch.Size([26791, 1, 250, 148])\n",
            "Human labeled\n",
            "Pet labeled\n",
            "IRobot labeled\n",
            "Human labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Pet labeled\n",
            "Fan labeled\n",
            "3\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Fan labeled\n",
            "3\n",
            "Human labeled\n",
            "Human labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "IRobot labeled\n",
            "IRobot labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Fan labeled\n",
            "3\n",
            "Human labeled\n",
            "Human labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "IRobot labeled\n",
            "IRobot labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n"
          ]
        }
      ],
      "source": [
        "task = 'FourClass'\n",
        "data_dir = \"/content/drive/MyDrive/DL_Sensing_Benchmark_OW/Data\"\n",
        "first_exp = \"1500_Intrusion_Eval\"\n",
        "BATCH_SIZE = 16\n",
        "DECAY_RATE = 0.001\n",
        "learning_rate = 1e-4\n",
        "win_len = 250\n",
        "feature_size = 148\n",
        "\n",
        "patience = 20\n",
        "num_epochs_1 = 100\n",
        "num_epochs_2 = 300\n",
        "\n",
        "#subexperiment_list = ['']\n",
        "# model_name_list =  [ \"MLP\",\"LeNet\", \"ResNet18\",\"ResNet50\",\"ResNet101\", \"GRU\",\"RNN\",\"LSTM\"]\n",
        "\n",
        "# model_name_list =  [ \"MLP\"] # \"MLP\"\n",
        "\n",
        "\n",
        "args.global_crops_scale = (args.min1, args.max1)\n",
        "args.local_crops_scale = (args.min2, args.max2)\n",
        "\n",
        "\n",
        "\n",
        "#for subexperiment in subexperiment_list:\n",
        "experiment = first_exp\n",
        "supervised_train_loader, supervised_val_loader, test_dataloader,unsupervised_train_loader = load_data(task, experiment, data_dir, BATCH_SIZE, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqwTdZlwF3pD"
      },
      "source": [
        "### start training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vUXpWmzT5xf",
        "outputId": "90fa64a1-6ea4-405b-8dff-efe5a5fd5c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using model: ViT_Parrallel\n",
            "925\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "DECAY_RATE = 0.001\n",
        "learning_rate = 1e-4\n",
        "win_len = 250\n",
        "feature_size = 148\n",
        "patience = 20\n",
        "num_epochs_1 = 100\n",
        "num_epochs_2 = 300\n",
        "depth = 6\n",
        "model_name_list =  [\"ViT\"]\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "criterion = InfoNCELoss(temperature=1)\n",
        "\n",
        "for model_name in model_name_list:\n",
        "  model = load_unsupervised_model(task, model_name,win_len,feature_size,depth=depth)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  save_path = \"/content/drive/MyDrive/DL_Sensing_Benchmark_OW/ExperimentLog/\"\\\n",
        "                  +experiment+\"/\"+task+\"_self_supervised_contrastive_simclr\"\\\n",
        "                  +\"/\"+model_name+\"/\"\n",
        "  if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AbSNQ4JsT7-W",
        "outputId": "29ebc822-ea19-4712-e7ba-03f028fca093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using model: ViT_Parrallel\n",
            "925\n",
            "Starting self-supervised training phase.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-4784728f73ff>:130: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  labels = torch.range(batch_size, 2*batch_size-1).long().to(z.device)\n",
            "<ipython-input-29-4784728f73ff>:131: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  labels = torch.cat([labels, torch.range(0, batch_size-1).long().to(z.device)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 2.7969\n",
            "Epoch 2/100, Train Loss: 2.5968\n",
            "Epoch 3/100, Train Loss: 2.5566\n",
            "Epoch 4/100, Train Loss: 2.5374\n",
            "Epoch 5/100, Train Loss: 2.5294\n",
            "Epoch 6/100, Train Loss: 2.5195\n",
            "Epoch 7/100, Train Loss: 2.5110\n",
            "Epoch 8/100, Train Loss: 2.4884\n",
            "Epoch 9/100, Train Loss: 2.4603\n",
            "Epoch 10/100, Train Loss: 2.4440\n",
            "Epoch 11/100, Train Loss: 2.4320\n",
            "Epoch 12/100, Train Loss: 2.4226\n",
            "Epoch 13/100, Train Loss: 2.4181\n",
            "Epoch 14/100, Train Loss: 2.4121\n",
            "Epoch 15/100, Train Loss: 2.4117\n",
            "Epoch 16/100, Train Loss: 2.4634\n",
            "Epoch 17/100, Train Loss: 2.4793\n",
            "Epoch 18/100, Train Loss: 2.4394\n",
            "Epoch 19/100, Train Loss: 2.4238\n",
            "Epoch 20/100, Train Loss: 2.4274\n",
            "Epoch 21/100, Train Loss: 2.4980\n",
            "Epoch 22/100, Train Loss: 2.4198\n",
            "Epoch 23/100, Train Loss: 2.4096\n",
            "Epoch 24/100, Train Loss: 2.4264\n",
            "Epoch 25/100, Train Loss: 3.6512\n",
            "Epoch 26/100, Train Loss: 3.6379\n",
            "Epoch 27/100, Train Loss: 3.5804\n",
            "Epoch 28/100, Train Loss: 3.5584\n",
            "Epoch 29/100, Train Loss: 3.5469\n",
            "Epoch 30/100, Train Loss: 3.5089\n",
            "Epoch 31/100, Train Loss: 3.5080\n",
            "Epoch 32/100, Train Loss: 3.4697\n",
            "Epoch 33/100, Train Loss: 3.4203\n",
            "Epoch 34/100, Train Loss: 3.4112\n",
            "Epoch 35/100, Train Loss: 3.3681\n",
            "Epoch 36/100, Train Loss: 3.3407\n",
            "Epoch 37/100, Train Loss: 3.3631\n",
            "Epoch 38/100, Train Loss: 3.3338\n",
            "Epoch 39/100, Train Loss: 3.3195\n",
            "Epoch 40/100, Train Loss: 3.2779\n",
            "Epoch 41/100, Train Loss: 3.2836\n",
            "Epoch 42/100, Train Loss: 3.3096\n",
            "Epoch 43/100, Train Loss: 3.2898\n",
            "Early stopping triggered after 20 epochs without improvement\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list indices must be integers or slices, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-16abee14912f>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mfig_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mfig_val_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"unsupervised_train_loss.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x700 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "DECAY_RATE = 0.001\n",
        "learning_rate = 1e-4\n",
        "win_len = 250\n",
        "feature_size = 148\n",
        "patience = 20\n",
        "num_epochs_1 = 100\n",
        "num_epochs_2 = 300\n",
        "depth = 6\n",
        "model_name_list =  [\"ViT\"]\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "criterion = InfoNCELoss(temperature=1)\n",
        "\n",
        "for model_name in model_name_list:\n",
        "  model = load_unsupervised_model(task, model_name,win_len,feature_size,depth=depth)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  save_path = \"/content/drive/MyDrive/DL_Sensing_Benchmark_OW/ExperimentLog/\"\\\n",
        "                  +experiment+\"/\"+task+\"_self_supervised_contrastive_simclr\"\\\n",
        "                  +\"/\"+model_name+\"/\"\n",
        "  if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "  # model, df1,df2 = train(model, unsupervised_train_loader, supervised_train_loader,supervised_val_loader,test_dataloader, num_epochs_1,num_epochs_2, learning_rate, criterion,ce_criterion, device, patience,save_path)\n",
        "  model, df1 =unsupervised_train(model, unsupervised_train_loader, num_epochs_1, learning_rate, criterion, device, patience, save_path)\n",
        "\n",
        "\n",
        "  fig_val_loss = plt.figure(figsize = (7,7))\n",
        "  sn.lineplot(x= df1['Epochs'], y = df1['total_loss'])\n",
        "  plt.show()\n",
        "  fig_val_loss.savefig(save_path+\"unsupervised_train_loss.png\")\n",
        "\n",
        "  fig_val_loss = plt.figure(figsize = (7,7))\n",
        "  sn.lineplot(x= df1['Epochs'], y = df1['kl_loss'])\n",
        "  plt.show()\n",
        "  fig_val_loss.savefig(save_path+\"unsupervised_kl_loss.png\")\n",
        "\n",
        "  fig_val_loss = plt.figure(figsize = (7,7))\n",
        "  sn.lineplot(x= df1['Epochs'], y = df1['he_loss'])\n",
        "  plt.show()\n",
        "  fig_val_loss.savefig(save_path+\"unsupervised_he_loss.png\")\n",
        "\n",
        "  fig_val_loss = plt.figure(figsize = (7,7))\n",
        "  sn.lineplot(x= df1['Epochs'], y = df1['eh_loss'])\n",
        "  plt.show()\n",
        "  fig_val_loss.savefig(save_path+\"unsupervised_eh_loss.png\")\n",
        "\n",
        "  fig_val_loss = plt.figure(figsize = (7,7))\n",
        "  sn.lineplot(x= df1['Epochs'], y = df1['kde_loss'])\n",
        "  plt.show()\n",
        "  fig_val_loss.savefig(save_path+\"unsupervised_kde_loss.png\")\n",
        "\n",
        "  df1.to_csv(save_path+\"unsupervised_train_loss.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df1['Epochs'], y = df1['total_loss'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"unsupervised_train_loss.png\")\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df1['Epochs'], y = df1['kl_loss'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"unsupervised_kl_loss.png\")\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df1['Epochs'], y = df1['he_loss'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"unsupervised_he_loss.png\")\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df1['Epochs'], y = df1['eh_loss'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"unsupervised_eh_loss.png\")\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df1['Epochs'], y = df1['kde_loss'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"unsupervised_kde_loss.png\")"
      ],
      "metadata": {
        "id": "yjfEZ6W8ngG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poVh-CT0T8Hb",
        "outputId": "bbabe151-4f52-4178-bb3a-979e4ec3bf10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using model: ViT_Parrallel\n",
            "925\n",
            "Starting supervised training phase.\n",
            "Epoch 1/300, Train Loss: 1.1656, Validation Loss: 1.1447\n",
            "Epoch 2/300, Train Loss: 1.0412, Validation Loss: 0.8078\n",
            "Epoch 3/300, Train Loss: 0.8740, Validation Loss: 0.7651\n",
            "Epoch 4/300, Train Loss: 0.7421, Validation Loss: 0.6571\n",
            "Epoch 5/300, Train Loss: 0.5889, Validation Loss: 0.4880\n",
            "Epoch 6/300, Train Loss: 0.5388, Validation Loss: 0.4332\n",
            "Epoch 7/300, Train Loss: 0.5060, Validation Loss: 0.6295\n",
            "Epoch 8/300, Train Loss: 0.5484, Validation Loss: 0.5015\n",
            "Epoch 9/300, Train Loss: 0.4925, Validation Loss: 0.4094\n",
            "Epoch 10/300, Train Loss: 0.5092, Validation Loss: 0.3704\n",
            "Epoch 11/300, Train Loss: 0.4659, Validation Loss: 0.4404\n",
            "Epoch 12/300, Train Loss: 0.5878, Validation Loss: 0.5204\n",
            "Epoch 13/300, Train Loss: 0.5557, Validation Loss: 0.4271\n",
            "Epoch 14/300, Train Loss: 0.5100, Validation Loss: 0.4173\n",
            "Epoch 15/300, Train Loss: 0.4922, Validation Loss: 0.6111\n",
            "Epoch 16/300, Train Loss: 0.5148, Validation Loss: 0.3745\n",
            "Epoch 17/300, Train Loss: 0.4728, Validation Loss: 0.3750\n",
            "Epoch 18/300, Train Loss: 0.4151, Validation Loss: 0.3412\n",
            "Epoch 19/300, Train Loss: 0.3765, Validation Loss: 0.3270\n",
            "Epoch 20/300, Train Loss: 0.3903, Validation Loss: 0.2975\n",
            "Epoch 21/300, Train Loss: 0.3814, Validation Loss: 0.3757\n",
            "Epoch 22/300, Train Loss: 0.3755, Validation Loss: 0.3134\n",
            "Epoch 23/300, Train Loss: 0.3691, Validation Loss: 0.2820\n",
            "Epoch 24/300, Train Loss: 0.3437, Validation Loss: 0.2746\n",
            "Epoch 25/300, Train Loss: 0.3445, Validation Loss: 0.2730\n",
            "Epoch 26/300, Train Loss: 0.3338, Validation Loss: 0.2765\n",
            "Epoch 27/300, Train Loss: 0.3434, Validation Loss: 0.2974\n",
            "Epoch 28/300, Train Loss: 0.3385, Validation Loss: 0.2901\n",
            "Epoch 29/300, Train Loss: 0.3470, Validation Loss: 0.2963\n",
            "Epoch 30/300, Train Loss: 0.3472, Validation Loss: 0.2940\n",
            "Epoch 31/300, Train Loss: 0.3427, Validation Loss: 0.2780\n",
            "Epoch 32/300, Train Loss: 0.3230, Validation Loss: 0.2951\n",
            "Epoch 33/300, Train Loss: 0.3224, Validation Loss: 0.2762\n",
            "Epoch 34/300, Train Loss: 0.3113, Validation Loss: 0.2677\n",
            "Epoch 35/300, Train Loss: 0.3137, Validation Loss: 0.2672\n",
            "Epoch 36/300, Train Loss: 0.3334, Validation Loss: 0.2755\n",
            "Epoch 37/300, Train Loss: 0.3162, Validation Loss: 0.2605\n",
            "Epoch 38/300, Train Loss: 0.3036, Validation Loss: 0.2557\n",
            "Epoch 39/300, Train Loss: 0.2988, Validation Loss: 0.2580\n",
            "Epoch 40/300, Train Loss: 0.2990, Validation Loss: 0.2549\n",
            "Epoch 41/300, Train Loss: 0.3019, Validation Loss: 0.2509\n",
            "Epoch 42/300, Train Loss: 0.2850, Validation Loss: 0.2898\n",
            "Epoch 43/300, Train Loss: 0.2890, Validation Loss: 0.2534\n",
            "Epoch 44/300, Train Loss: 0.2835, Validation Loss: 0.2585\n",
            "Epoch 45/300, Train Loss: 0.2846, Validation Loss: 0.2470\n",
            "Epoch 46/300, Train Loss: 0.2778, Validation Loss: 0.2680\n",
            "Epoch 47/300, Train Loss: 0.2817, Validation Loss: 0.2473\n",
            "Epoch 48/300, Train Loss: 0.2763, Validation Loss: 0.2497\n",
            "Epoch 49/300, Train Loss: 0.2748, Validation Loss: 0.2506\n",
            "Epoch 50/300, Train Loss: 0.2721, Validation Loss: 0.2479\n",
            "Epoch 51/300, Train Loss: 0.2728, Validation Loss: 0.2440\n",
            "Epoch 52/300, Train Loss: 0.2699, Validation Loss: 0.2431\n",
            "Epoch 53/300, Train Loss: 0.2674, Validation Loss: 0.2478\n",
            "Epoch 54/300, Train Loss: 0.2680, Validation Loss: 0.2491\n",
            "Epoch 55/300, Train Loss: 0.2673, Validation Loss: 0.2367\n",
            "Epoch 56/300, Train Loss: 0.2651, Validation Loss: 0.2395\n",
            "Epoch 57/300, Train Loss: 0.2667, Validation Loss: 0.2392\n",
            "Epoch 58/300, Train Loss: 0.2636, Validation Loss: 0.2414\n",
            "Epoch 59/300, Train Loss: 0.2663, Validation Loss: 0.2372\n",
            "Epoch 60/300, Train Loss: 0.2625, Validation Loss: 0.2559\n",
            "Epoch 61/300, Train Loss: 0.2687, Validation Loss: 0.2411\n",
            "Epoch 62/300, Train Loss: 0.2644, Validation Loss: 0.2342\n",
            "Epoch 63/300, Train Loss: 0.2587, Validation Loss: 0.2404\n",
            "Epoch 64/300, Train Loss: 0.2616, Validation Loss: 0.2337\n",
            "Epoch 65/300, Train Loss: 0.2595, Validation Loss: 0.2341\n",
            "Epoch 66/300, Train Loss: 0.2594, Validation Loss: 0.2324\n",
            "Epoch 67/300, Train Loss: 0.2577, Validation Loss: 0.2315\n",
            "Epoch 68/300, Train Loss: 0.2571, Validation Loss: 0.2331\n",
            "Epoch 69/300, Train Loss: 0.2581, Validation Loss: 0.2304\n",
            "Epoch 70/300, Train Loss: 0.2575, Validation Loss: 0.2304\n",
            "Epoch 71/300, Train Loss: 0.2567, Validation Loss: 0.2295\n",
            "Epoch 72/300, Train Loss: 0.2560, Validation Loss: 0.2348\n",
            "Epoch 73/300, Train Loss: 0.2561, Validation Loss: 0.2318\n",
            "Epoch 74/300, Train Loss: 0.2549, Validation Loss: 0.2336\n",
            "Epoch 75/300, Train Loss: 0.2529, Validation Loss: 0.2287\n",
            "Epoch 76/300, Train Loss: 0.2572, Validation Loss: 0.2302\n",
            "Epoch 77/300, Train Loss: 0.2567, Validation Loss: 0.2298\n",
            "Epoch 78/300, Train Loss: 0.2536, Validation Loss: 0.2289\n",
            "Epoch 79/300, Train Loss: 0.2510, Validation Loss: 0.2282\n",
            "Epoch 80/300, Train Loss: 0.2535, Validation Loss: 0.2293\n",
            "Epoch 81/300, Train Loss: 0.2549, Validation Loss: 0.2280\n",
            "Epoch 82/300, Train Loss: 0.2502, Validation Loss: 0.2284\n",
            "Epoch 83/300, Train Loss: 0.2513, Validation Loss: 0.2285\n",
            "Epoch 84/300, Train Loss: 0.2529, Validation Loss: 0.2278\n",
            "Epoch 85/300, Train Loss: 0.2483, Validation Loss: 0.2279\n",
            "Epoch 86/300, Train Loss: 0.2541, Validation Loss: 0.2278\n",
            "Epoch 87/300, Train Loss: 0.2506, Validation Loss: 0.2297\n",
            "Epoch 88/300, Train Loss: 0.2503, Validation Loss: 0.2276\n",
            "Epoch 89/300, Train Loss: 0.2515, Validation Loss: 0.2294\n",
            "Epoch 90/300, Train Loss: 0.2513, Validation Loss: 0.2298\n",
            "Epoch 91/300, Train Loss: 0.2541, Validation Loss: 0.2270\n",
            "Epoch 92/300, Train Loss: 0.2514, Validation Loss: 0.2276\n",
            "Epoch 93/300, Train Loss: 0.2493, Validation Loss: 0.2273\n",
            "Epoch 94/300, Train Loss: 0.2517, Validation Loss: 0.2278\n",
            "Epoch 95/300, Train Loss: 0.2499, Validation Loss: 0.2287\n",
            "Epoch 96/300, Train Loss: 0.2483, Validation Loss: 0.2274\n",
            "Epoch 97/300, Train Loss: 0.2499, Validation Loss: 0.2265\n",
            "Epoch 98/300, Train Loss: 0.2483, Validation Loss: 0.2258\n",
            "Epoch 99/300, Train Loss: 0.2478, Validation Loss: 0.2281\n",
            "Epoch 100/300, Train Loss: 0.2481, Validation Loss: 0.2260\n",
            "Epoch 101/300, Train Loss: 0.2480, Validation Loss: 0.2270\n",
            "Epoch 102/300, Train Loss: 0.2476, Validation Loss: 0.2267\n",
            "Epoch 103/300, Train Loss: 0.2476, Validation Loss: 0.2275\n",
            "Epoch 104/300, Train Loss: 0.2512, Validation Loss: 0.2261\n",
            "Epoch 105/300, Train Loss: 0.2464, Validation Loss: 0.2260\n",
            "Epoch 106/300, Train Loss: 0.2471, Validation Loss: 0.2268\n",
            "Epoch 107/300, Train Loss: 0.2484, Validation Loss: 0.2261\n",
            "Epoch 108/300, Train Loss: 0.2518, Validation Loss: 0.2260\n",
            "Epoch 109/300, Train Loss: 0.2482, Validation Loss: 0.2264\n",
            "Epoch 110/300, Train Loss: 0.2507, Validation Loss: 0.2261\n",
            "Epoch 111/300, Train Loss: 0.2466, Validation Loss: 0.2260\n",
            "Epoch 112/300, Train Loss: 0.2477, Validation Loss: 0.2254\n",
            "Epoch 113/300, Train Loss: 0.2458, Validation Loss: 0.2260\n",
            "Epoch 114/300, Train Loss: 0.2490, Validation Loss: 0.2264\n",
            "Epoch 115/300, Train Loss: 0.2452, Validation Loss: 0.2258\n",
            "Epoch 116/300, Train Loss: 0.2498, Validation Loss: 0.2269\n",
            "Epoch 117/300, Train Loss: 0.2471, Validation Loss: 0.2264\n",
            "Epoch 118/300, Train Loss: 0.2484, Validation Loss: 0.2260\n",
            "Epoch 119/300, Train Loss: 0.2466, Validation Loss: 0.2263\n",
            "Epoch 120/300, Train Loss: 0.2466, Validation Loss: 0.2260\n",
            "Epoch 121/300, Train Loss: 0.2495, Validation Loss: 0.2261\n",
            "Epoch 122/300, Train Loss: 0.2500, Validation Loss: 0.2260\n",
            "Epoch 123/300, Train Loss: 0.2480, Validation Loss: 0.2262\n",
            "Epoch 124/300, Train Loss: 0.2473, Validation Loss: 0.2257\n",
            "Epoch 125/300, Train Loss: 0.2488, Validation Loss: 0.2257\n",
            "Epoch 126/300, Train Loss: 0.2447, Validation Loss: 0.2259\n",
            "Epoch 127/300, Train Loss: 0.2446, Validation Loss: 0.2259\n",
            "Epoch 128/300, Train Loss: 0.2457, Validation Loss: 0.2266\n",
            "Epoch 129/300, Train Loss: 0.2466, Validation Loss: 0.2264\n",
            "Epoch 130/300, Train Loss: 0.2445, Validation Loss: 0.2265\n",
            "Epoch 131/300, Train Loss: 0.2459, Validation Loss: 0.2265\n",
            "Epoch 132/300, Train Loss: 0.2448, Validation Loss: 0.2262\n",
            "Epoch 133/300, Train Loss: 0.2501, Validation Loss: 0.2260\n",
            "Epoch 134/300, Train Loss: 0.2473, Validation Loss: 0.2260\n",
            "Epoch 135/300, Train Loss: 0.2476, Validation Loss: 0.2261\n",
            "Epoch 136/300, Train Loss: 0.2451, Validation Loss: 0.2262\n",
            "Epoch 137/300, Train Loss: 0.2453, Validation Loss: 0.2261\n",
            "Epoch 138/300, Train Loss: 0.2471, Validation Loss: 0.2261\n",
            "Epoch 139/300, Train Loss: 0.2460, Validation Loss: 0.2260\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "DECAY_RATE = 0.001\n",
        "learning_rate = 1e-4\n",
        "win_len = 250\n",
        "feature_size = 148\n",
        "patience = 20\n",
        "num_epochs_1 = 100\n",
        "num_epochs_2 = 300\n",
        "depth = 6\n",
        "model_name_list =  [\"ViT\"]\n",
        "ce_criterion = nn.CrossEntropyLoss()\n",
        "criterion = InfoNCELoss(temperature=1)\n",
        "\n",
        "for model_name in model_name_list:\n",
        "  model = load_unsupervised_model(task, model_name,win_len,feature_size,depth=depth)\n",
        "\n",
        "checkpoint_path = save_path+\"best_model_checkpoint_ssl.pth.tar\"\n",
        "model, start_epoch, last_loss = load_checkpoint(checkpoint_path, model)\n",
        "\n",
        "model,df2 = train_supervised(model, supervised_train_loader, supervised_val_loader, test_dataloader, num_epochs_2, learning_rate, ce_criterion, device, optimizer_decay_rate = DECAY_RATE, patience=patience,save_path=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckjVehWLT8LT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "b64bace2-719b-4811-fbb4-8a38b2071185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 71.05%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-dfe2ad1b20c8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlineplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfig_val_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"supervised_train_loss.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x700 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_test = test(model, test_dataloader,task,save_path)\n",
        "\n",
        "\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df2['Epochs'], y = df2['train_loss'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"supervised_train_loss.png\")\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df2['Epochs'], y = df2['validation_loss'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"supervised_validation_loss.png\")\n",
        "\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df2['Epochs'], y = df2['supervised_train_acc_1'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"supervised_train_acc_1.png\")\n",
        "\n",
        "fig_val_loss = plt.figure(figsize = (7,7))\n",
        "sn.lineplot(x= df2['Epochs'], y = df2['supervised_train_acc_2'])\n",
        "plt.show()\n",
        "fig_val_loss.savefig(save_path+\"supervised_train_acc_2.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spPl0tQ-eWkj",
        "outputId": "f6447c39-bc83-4e54-f548-2a7ae9b0b68d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IRobot labeled\n",
            "Pet labeled\n",
            "Fan labeled\n",
            "3\n",
            "Human labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "Human labeled\n",
            "IRobot labeled\n",
            "IRobot labeled\n",
            "Pet labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n",
            "Human labeled\n"
          ]
        }
      ],
      "source": [
        "task = 'FourClass'\n",
        "data_dir = \"/content/drive/MyDrive/DL_Sensing_Benchmark_OW/Data\"\n",
        "experiment = \"1500_Intrusion_Eval\"\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "# get the class number of different task\n",
        "classes = {'HumanNonhuman': 2, 'FourClass': 4, 'HumanID': 4, 'HumanMotion': 3, 'ThreeClass': 3}\n",
        "\n",
        "# retrive the data\n",
        "train_set = OW_dataset_class(data_dir, task, experiment, 0, 1)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcw0nl4VT8Og",
        "outputId": "89eecc2b-9cf1-44d7-bd79-670db072dd58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.0064])\n",
            "tensor([0.0217])\n"
          ]
        }
      ],
      "source": [
        "mean = 0\n",
        "std = 0\n",
        "\n",
        "for x,y in train_loader:\n",
        "    batch_samples = x.size(0) # batch size (the last batch can have smaller size!)\n",
        "    x = x.view(batch_samples, x.size(1), -1)\n",
        "    mean += x.mean(2).sum(0)\n",
        "    std += x.std(2).sum(0)\n",
        "\n",
        "mean /= len(train_loader.dataset)\n",
        "std /= len(train_loader.dataset)\n",
        "\n",
        "print(mean)\n",
        "print(std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfcviOD3T8RG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waKcLdotT8UU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lD_0cSpTT8XM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TNl0VX3T8aD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssG-b00Mn7uB"
      },
      "outputs": [],
      "source": [
        "# #######################################\n",
        "# # self-supervised training\n",
        "# print ('Self-supervised encoder training')\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=args.weight_decay)\n",
        "# for epoch in range(100):\n",
        "#         total_loss = 0\n",
        "#         kl_loss = 0\n",
        "#         eh_loss = 0\n",
        "#         he_loss = 0\n",
        "#         kde_loss = 0\n",
        "#         for data in unsupervised_train_loader:\n",
        "#                 x, y = data\n",
        "#                 x, y = x.to(device), y.to(device)\n",
        "#                 x1 = gaussian_noise(x, random.uniform(0, 2.0),win_len,feature_size)\n",
        "#                 x2 = gaussian_noise(x, random.uniform(0.1, 2.0),win_len,feature_size)\n",
        "\n",
        "#                 # ===================forward=====================\n",
        "#                 feat_x1, feat_x2 = model(x1, x2)\n",
        "#                 loss = criterion(feat_x1, feat_x2)\n",
        "#                 loss_kl = loss['kl']\n",
        "#                 loss_eh = loss['eh']\n",
        "#                 loss_he = loss['he']\n",
        "#                 loss_kde = loss['kde']\n",
        "#                 loss = loss['final-kde']\n",
        "\n",
        "#                 # ===================backward====================\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#                 # ===================log========================\n",
        "#                 total_loss += loss.data\n",
        "#                 kl_loss += loss_kl.data\n",
        "#                 eh_loss += loss_eh.data\n",
        "#                 he_loss += loss_he.data\n",
        "#                 kde_loss += loss_kde.data\n",
        "#         print('epoch [{}/{}], total loss:{:.4f},kl loss:{:.4f},eh loss:{:.4f},he loss:{:.4f},kde loss:{:.4f}'\n",
        "#                 .format(epoch+1,100, total_loss, kl_loss, eh_loss, he_loss, kde_loss))\n",
        "# #######################################\n",
        "\n",
        "# #######################################\n",
        "# # test\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     correct_1, correct_2 = 0, 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for data in test_dataloader:\n",
        "#             x, y = data\n",
        "#             x, y = x.to(device), y.to(device)\n",
        "\n",
        "#             y1, y2 = model(x, x, flag='supervised')\n",
        "#             _, pred_1 = torch.max(y1.data, 1)\n",
        "#             _, pred_2 = torch.max(y2.data, 1)\n",
        "#             total += y.size(0)\n",
        "#             correct_1 += (pred_1 == y).sum().item()\n",
        "#             correct_2 += (pred_2 == y).sum().item()\n",
        "\n",
        "#     print('Test accuracy: {:.2f}%, {:.2f}%'.format(100 * correct_1 / total, 100 * correct_2 / total))\n",
        "# #######################################\n",
        "\n",
        "# ##################################\n",
        "# # supervised learning\n",
        "# print ('Supervised classifier training')\n",
        "# optimizer_supervised = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "# for epoch in range(300):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for data in supervised_train_loader:\n",
        "#         x, y = data\n",
        "#         x = Variable(x).to(device)\n",
        "#         y = y.type(torch.LongTensor)\n",
        "#         y = y.to(device)\n",
        "\n",
        "#         # ===================forward=====================\n",
        "#         y1, y2 = model(x, x, flag='supervised')\n",
        "#         loss = ce_criterion(y1, y) + ce_criterion(y2, y)\n",
        "\n",
        "#         # ===================backward====================\n",
        "#         optimizer_supervised.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer_supervised.step()\n",
        "#     # ===================log========================\n",
        "#     total_loss += loss.data\n",
        "#     print('epoch [{}/{}], loss:{:.6f}'\n",
        "#         .format(epoch+1, 300, total_loss))\n",
        "#     # test\n",
        "#     if epoch > 250:\n",
        "#         test()\n",
        "# ##################################\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "l34KhClRxihO",
        "ns2hUDV8dwpf",
        "Uq_r1gAZnUVX"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}